{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T11:33:43.289449Z",
     "start_time": "2025-03-30T11:33:43.259703Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer,AutoModelForMaskedLM, AutoModelForCausalLM,AutoModelForSeq2SeqLM,GraphormerForGraphClassification\n",
    "import pubchempy as pcp\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from rdkit import Chem\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../../../../T5 EVO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d78a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(seed=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "430dacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_representations(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    model_name: str,   \n",
    "    ds: str,\n",
    "    input_type: str = \"smiles\",   # \"smiles\" or \"selfies\"\n",
    "    token: int = 0,               # which token to read (e.g., CLS/first)\n",
    "    save_path: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create embeddings from UNIQUE CIDs using either SMILES or SELFIES.\n",
    "    The output DF has:\n",
    "        ['cid', 'isomeric_text', 'canonical_text', 'input_type', 'model', 'layer', 'e0', ..., 'e{d-1}'].\n",
    "\n",
    "    Tokenization uses the chosen input_type; for each row it prefers the isomeric text if present,\n",
    "    otherwise falls back to the canonical text. Both text variants are kept as metadata columns.\n",
    "    \"\"\"\n",
    "    assert input_type.lower() in {\"smiles\", \"selfies\"}, \"input_type must be 'smiles' or 'selfies'\"\n",
    "    model.eval()\n",
    "\n",
    "    # ---- Load & pick columns ----\n",
    "    df = pd.read_csv(f\"datasets/{ds}/{ds}_data.csv\")\n",
    "    if \"cid\" not in df.columns:\n",
    "        raise ValueError(\"Dataset must contain a 'cid' column.\")\n",
    "\n",
    "    if input_type.lower() == \"smiles\":\n",
    "        iso_col = \"isomericsmiles\"\n",
    "        can_col = \"canonicalsmiles\"\n",
    "    else:  # selfies\n",
    "        iso_col = \"isomericselfies\"\n",
    "        can_col = \"canonicalselfies\"\n",
    "\n",
    "    if iso_col is None and can_col is None:\n",
    "        raise ValueError(f\"No {input_type} columns found.\")\n",
    "\n",
    "    use_cols = [\"cid\"]\n",
    "    if iso_col: use_cols.append(iso_col)\n",
    "    if can_col: use_cols.append(can_col)\n",
    "\n",
    "    work = df[use_cols].copy()\n",
    "    work = work.drop_duplicates(subset=[\"cid\"], keep=\"first\").sort_values(\"cid\").reset_index(drop=True)\n",
    "\n",
    "   \n",
    "    # ---- Device ----\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    rows = []\n",
    "    emb_dim = None\n",
    "\n",
    "    for cid, iso_txt, can_txt in zip(work[\"cid\"], work[iso_col], work[can_col]):\n",
    "        # --- normalize texts ---\n",
    "        iso_txt = iso_txt if isinstance(iso_txt, str) and iso_txt.strip() else \"\"\n",
    "        can_txt = can_txt if isinstance(can_txt, str) and can_txt.strip() else \"\"\n",
    "        if not iso_txt and not can_txt:\n",
    "            continue  # nothing to encode for this cid\n",
    "\n",
    "        # --- run model for isomeric (if present) ---\n",
    "        iso_hiddens = None\n",
    "        if iso_txt:\n",
    "            iso_inputs = tokenizer([iso_txt], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            iso_inputs = {k: v.to(device) for k, v in iso_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                iso_out = model(**iso_inputs, output_hidden_states=True)\n",
    "            iso_hiddens = iso_out.hidden_states  # tuple of [B,T,D] tensors\n",
    "\n",
    "        # --- run model for canonical (if present) ---\n",
    "        can_hiddens = None\n",
    "        if can_txt:\n",
    "            can_inputs = tokenizer([can_txt], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            can_inputs = {k: v.to(device) for k, v in can_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                can_out = model(**can_inputs, output_hidden_states=True)\n",
    "            can_hiddens = can_out.hidden_states\n",
    "\n",
    "        # number of layers to emit = max of the two (they should match)\n",
    "        n_layers = max(\n",
    "            len(iso_hiddens) if iso_hiddens is not None else 0,\n",
    "            len(can_hiddens) if can_hiddens is not None else 0,\n",
    "        )\n",
    "\n",
    "        for layer_idx in range(n_layers):\n",
    "            # get vectors (or None) for this layer\n",
    "           \n",
    "            iso_vec = iso_hiddens[layer_idx][0, token, :].detach().cpu().numpy()\n",
    "\n",
    "            \n",
    "            can_vec = can_hiddens[layer_idx][0, token, :].detach().cpu().numpy()\n",
    "\n",
    "            # set / check embedding dim\n",
    "            if emb_dim is None:\n",
    "                \n",
    "                emb_dim = iso_vec.shape[0]\n",
    "                emb_dim = can_vec.shape[0]\n",
    "            # sanity: if both exist, ensure same D\n",
    "            if (iso_vec is not None) and (can_vec is not None):\n",
    "                assert iso_vec.shape[0] == can_vec.shape[0], \"Iso/Can dims differ!\"\n",
    "\n",
    "            row = {\n",
    "                \"cid\": cid,\n",
    "                \"isomeric_text\": iso_txt,\n",
    "                \"canonical_text\": can_txt,\n",
    "                \"input_type\": input_type.lower(),   # \"smiles\" or \"selfies\"\n",
    "                \"model\": model_name,\n",
    "                \"layer\": layer_idx,\n",
    "            }\n",
    "\n",
    "            # add iso_* columns (fill with NaN if missing)\n",
    "            if emb_dim is None:\n",
    "                continue  # defensive; should not happen if any vec exists\n",
    "            for i in range(emb_dim):\n",
    "                row[f\"iso_e{i}\"] = float(iso_vec[i]) \n",
    "                row[f\"can_e{i}\"] = float(can_vec[i])\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "\n",
    "        out_df = pd.DataFrame(rows)\n",
    "\n",
    "        \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    out_df.to_csv(f\"{save_path}/{ds}_{model_name.split('/')[1]}_embeddings.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ab8fabf3601a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T07:32:48.485874Z",
     "start_time": "2025-03-31T07:32:48.439231Z"
    }
   },
   "outputs": [],
   "source": [
    "# def extract_representations(tokenizer, model,model_name,input_type='smiles',token=0):\n",
    "#     model.eval()  \n",
    "#     for subject_id in range(s_start,s_end+1):\n",
    "#         input_molecules = pd.read_csv(f'{base_dir}/datasets/{ds}/{ds}_data.csv')[input_type].values.tolist()\n",
    "#         inputs = tokenizer(input_molecules, padding=True, return_tensors=\"pt\")\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs,output_hidden_states=True)\n",
    "#             for i,output in enumerate(outputs.hidden_states):\n",
    "#                 np.save(f'{base_dir}/fmri/embeddings{ds}/embeddings_{model_name}_{subject_id}_{i}{ds}.npy', output[:,token,:].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca77193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_representations_by_molfeat(model_name,transformer,input_type='smiles',token=0):\n",
    "    \n",
    "#     for subject_id in range(s_start,s_end+1):\n",
    "#         input_molecules = pd.read_csv(f'{base_dir}/fmri/embeddings{ds}/CIDs_smiles_selfies_{subject_id}{ds}.csv')[input_type].values.tolist()\n",
    "        \n",
    "#         outputs = transformer(input_molecules)\n",
    "#         np.save(f'{base_dir}/fmri/embeddings{ds}/embeddings_{model_name}_{subject_id}_{-1}{ds}.npy',outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e1f67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e49e21f0d63ede",
   "metadata": {},
   "source": [
    "# Encoder-Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c357405615704",
   "metadata": {},
   "source": [
    "## MoLFormer-XL-both-10pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9682defb4c9030c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T21:03:57.047530Z",
     "start_time": "2025-01-06T21:00:03.803379Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/var/folders/s8/bznqh0v13ddg31lz919tkrmc0000gn/T/ipykernel_37065/1033378615.py:22: DtypeWarning: Columns (166) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"datasets/{ds}/{ds}_data.csv\")\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/var/folders/s8/bznqh0v13ddg31lz919tkrmc0000gn/T/ipykernel_37065/1033378615.py:22: DtypeWarning: Columns (166) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"datasets/{ds}/{ds}_data.csv\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jonghyunlee/ChemBERT_ChEMBL_pretrained and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jonghyunlee/ChemBERT_ChEMBL_pretrained and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jonghyunlee/ChemBERT_ChEMBL_pretrained and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/s8/bznqh0v13ddg31lz919tkrmc0000gn/T/ipykernel_37065/1033378615.py:22: DtypeWarning: Columns (166) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"datasets/{ds}/{ds}_data.csv\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at HUBioDataLab/SELFormer and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at HUBioDataLab/SELFormer and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at HUBioDataLab/SELFormer and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/s8/bznqh0v13ddg31lz919tkrmc0000gn/T/ipykernel_37065/1033378615.py:22: DtypeWarning: Columns (166) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"datasets/{ds}/{ds}_data.csv\")\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "Input_types = {\n",
    "    'ibm/MoLFormer-XL-both-10pct':'smiles',\n",
    "    'seyonec/ChemBERTa-zinc-base-v1':'smiles',\n",
    "    'jonghyunlee/ChemBERT_ChEMBL_pretrained':'smiles',\n",
    "    \"HUBioDataLab/SELFormer\":'selfies'\n",
    "    }\n",
    "\n",
    "for model_name in ['ibm/MoLFormer-XL-both-10pct','seyonec/ChemBERTa-zinc-base-v1',\"jonghyunlee/ChemBERT_ChEMBL_pretrained\",\"HUBioDataLab/SELFormer\"]:\n",
    "    for ds in ['sagar2023','keller2016','bierling2025']:\n",
    "    \n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        extract_representations(tokenizer, model,model_name,save_path='embeddings',ds=ds,input_type=Input_types[model_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53faca498e649c",
   "metadata": {},
   "source": [
    "## ChemBERTa-zinc-base-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348c663042dd61b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T21:14:56.602147Z",
     "start_time": "2025-01-06T21:10:54.423010Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "extract_representations(tokenizer, model,'ChemBERTa-zinc-base-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db54b60b6dcd2ef",
   "metadata": {},
   "source": [
    "## SELFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20437810d791dbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T12:40:29.362714Z",
     "start_time": "2025-01-09T12:36:45.936238Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HUBioDataLab/SELFormer\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"HUBioDataLab/SELFormer\")\n",
    "extract_representations(tokenizer, model,'SELFormer',input_type='selfies')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb26e631615206c",
   "metadata": {},
   "source": [
    "## ChemBERT_ChEMBL_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c118234e621183",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T14:00:47.967987Z",
     "start_time": "2025-03-31T14:00:40.373439Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"jonghyunlee/ChemBERT_ChEMBL_pretrained\")\n",
    "model = AutoModel.from_pretrained(\"jonghyunlee/ChemBERT_ChEMBL_pretrained\")\n",
    "extract_representations(tokenizer, model,'ChemBERT_ChEMBL_pretrained')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec9b602c4e3feb",
   "metadata": {},
   "source": [
    "# Decoder-Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0fb2f2199db515",
   "metadata": {},
   "source": [
    "## BARTSmiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9edb270a70d32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T15:31:02.400943Z",
     "start_time": "2025-03-31T15:23:43.638026Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"gayane/\"\n",
    "model_name = \"BARTSmiles\"  # Replace with actual model name if different\n",
    "model = AutoModel.from_pretrained(model_path+model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path+model_name,add_prefix_space=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model.eval()\n",
    "for subject_id in range(1, 4):\n",
    "        \n",
    "        \n",
    "        input_molecules = pd.read_csv(f'{base_dir}/fmri/embeddings/CIDs_smiles_selfies_{subject_id}.csv')['smiles'].values.tolist()\n",
    "        inputs = tokenizer(input_molecules, return_tensors=\"pt\",return_token_type_ids=False, add_special_tokens=True,padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs,output_hidden_states=True)\n",
    "            for i,output in enumerate(outputs.decoder_hidden_states):\n",
    "                np.save(f'{base_dir}/fmri/embeddings/embeddings_decoder_{model_name}_{subject_id}_{i}.npy', output[:,-1,:].cpu().numpy())\n",
    "                print(i,output.shape)\n",
    "                # np.save(f'{base_dir}/fmri/results/decoder_{model_name}_{subject_id}_{i}_avg.npy', output[:,:].cpu().numpy())\n",
    "\n",
    "\n",
    "            for i,output in enumerate(outputs.encoder_hidden_states):\n",
    "                np.save(f'{base_dir}/fmri/embeddings/embeddings_encoder_{model_name}_{subject_id}_{i}.npy', output[:,0,:].cpu().numpy())\n",
    "                print(i,output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e1cc0038d986",
   "metadata": {},
   "source": [
    "## SMILES-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f07312fb298a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:41:04.673838Z",
     "start_time": "2025-04-01T07:40:51.208766Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "model_name= 'smiles-gpt'\n",
    "model_dir = f'{base_dir}/fmri/models/smiles-gpt/'\n",
    "checkpoint = \"checkpoints/benchmark-5m\"\n",
    "\n",
    "config = GPT2Config.from_pretrained(model_dir+checkpoint, output_hidden_states=True)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_dir+checkpoint, config=config)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_dir+checkpoint,add_prefix_space=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.eval()\n",
    "for subject_id in range(1, 4):\n",
    "        input_molecules = pd.read_csv(f'{base_dir}/fmri/embeddings/CIDs_smiles_selfies_{subject_id}.csv')['smiles'].values.tolist()\n",
    "        inputs = tokenizer(input_molecules, return_tensors=\"pt\", add_special_tokens=True,return_token_type_ids=False,padding=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs,return_dict=True)\n",
    "            for i,output in enumerate(outputs.hidden_states):\n",
    "                np.save(f'{base_dir}/fmri/embeddings/embeddings_{model_name}_{subject_id}_{i}.npy', output[:,-1,:].cpu().numpy())\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c6f93f37cb03b",
   "metadata": {},
   "source": [
    "# MoLGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7ad386bac15df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:02:21.095935Z",
     "start_time": "2025-04-01T07:59:29.833149Z"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# model_path = \"zjunlp/\"\n",
    "# model_name = \"MolGen-large\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path+model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_path+model_name)\n",
    "# model.eval()\n",
    "# for subject_id in range(1, 4):\n",
    "#         CIDs, smiles_subject = read_CIDs(base_dir,subject_id)\n",
    "#         # smiles_subject2=['CC(C)CC1=CC=C(C=C1)C(C)C(=O)O','CC(C)CC1=CC=C(C=C1)C(C)C(=O']\n",
    "#         inputs = tokenizer(smiles_subject, return_tensors=\"pt\",return_token_type_ids=False, add_special_tokens=True,padding=True)\n",
    "#         # inputs.pop(\"token_type_ids\", None)\n",
    "#         # print(type(smiles_subject), smiles_subject)\n",
    "#         print(tokenizer.vocab_size)\n",
    "#         print(tokenizer.tokenize(smiles_subject[0]))\n",
    "\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs,output_hidden_states=True)\n",
    "#             for i,output in enumerate(outputs.decoder_hidden_states):\n",
    "#                 print(i,output.shape)\n",
    "#                 np.save(f'results/embeddings_decoder_{model_name}_{subject_id}_{i}.npy', output[:,-1,:].cpu().numpy())\n",
    "#                 output = torch.mean(output, dim=1)\n",
    "#                 np.save(f'results/embeddings_decoder_{model_name}_{subject_id}_{i}_avg.npy', output[:,:].cpu().numpy())\n",
    "\n",
    "\n",
    "#             for i,output in enumerate(outputs.encoder_hidden_states):\n",
    "#                 print(i,output.shape)\n",
    "#                 np.save(f'results/embeddings_encoder_{model_name}_{subject_id}_{i}.npy', output[:,-1,:].cpu().numpy())\n",
    "#                 output = torch.mean(output, dim=1)\n",
    "#                 np.save(f'results/embeddings_encoder_{model_name}_{subject_id}_{i}_avg.npy', output[:,:].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6539ef6",
   "metadata": {},
   "source": [
    "## ChemGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ncfrey/ChemGPT-4.7M\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ncfrey/ChemGPT-4.7M\")\n",
    "extract_representations(tokenizer, model,'ChemGPT-4.7M',token=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ncfrey/ChemGPT-19M\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ncfrey/ChemGPT-19M\")\n",
    "extract_representations(tokenizer, model,'ChemGPT-19M',token=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ncfrey/ChemGPT-1.2B\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ncfrey/ChemGPT-1.2B\")\n",
    "extract_representations(tokenizer, model,'ChemGPT-1.2B',token=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733e3580e10d01e",
   "metadata": {},
   "source": [
    "# MoLGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267856ef09dcdd61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T13:08:35.822332Z",
     "start_time": "2025-01-09T13:04:51.943369Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"msb-roshan/molgpt\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"msb-roshan/molgpt\")\n",
    "extract_representations(tokenizer, model,'molgpt',token=-1,input_type='selfies')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42ec3f4abee951d",
   "metadata": {},
   "source": [
    "# IBM SmallMoleculeMultiViewModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4162ec01ff0121bf",
   "metadata": {},
   "source": [
    "# GTMGC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242553220c7a9e9b",
   "metadata": {},
   "source": [
    "# Molecular Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690da1ab40ecfe0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:03:52.395320Z",
     "start_time": "2025-03-28T13:00:27.194625Z"
    }
   },
   "outputs": [],
   "source": [
    "descriptors =pd.read_csv(f'{base_dir}/fmri/molecular_descriptors_data.txt', sep='\\t')\n",
    "descriptors.set_index('CID', inplace=True)\n",
    "descriptors.sort_values(by='CID',inplace=True)\n",
    "descriptors.fillna(value=0,inplace=True)\n",
    "for subject_id in range(1, 4):\n",
    "    CIDs - pd.read_csv(f'{base_dir}/fmri/embeddings/CIDs_smiles_selfies_{subject_id}.csv')['CIDs'].values\n",
    "    descriptors_cid = descriptors.loc[CIDs]\n",
    "    descriptors_numpy = descriptors_cid.to_numpy()\n",
    "    np.save(f'{base_dir}/fmri/embeddings/embeddings_molecular_descriptors_{subject_id}_1.npy', descriptors_numpy)\n",
    "\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9debc58522956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T12:38:40.613283Z",
     "start_time": "2025-03-28T12:38:40.564878Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#convert dataframe to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccedb3f6adf5a729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T12:39:42.901070Z",
     "start_time": "2025-03-28T12:39:42.865268Z"
    }
   },
   "outputs": [],
   "source": [
    "descriptors_cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46b8db11fdafbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T12:38:44.609373Z",
     "start_time": "2025-03-28T12:38:44.598882Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`graphormer` is required to use this featurizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmolfeat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpretrained\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphormerTransformer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# import datamol as dm\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m transformer \u001b[38;5;241m=\u001b[39m \u001b[43mGraphormerTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpcqm4mv2_graphormer_base\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# smiles = dm.freesolv().iloc[:100].smiles\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# extract_representations_by_molfeat('pcqm4mv2_graphormer_base',transformer,input_type='smiles',token=-1)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/graphormer38/lib/python3.8/site-packages/molfeat/trans/pretrained/graphormer.py:65\u001b[0m, in \u001b[0;36mGraphormerTransformer.__init__\u001b[0;34m(self, kind, dtype, pooling, max_length, concat_layers, ignore_padding, version, **params)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mdtype, pooling\u001b[38;5;241m=\u001b[39mpooling, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m requires\u001b[38;5;241m.\u001b[39mcheck(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraphormer_pretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`graphormer` is required to use this featurizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_layers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     concat_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: `graphormer` is required to use this featurizer."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "# np.float = float \n",
    "import sys\n",
    "# sys.path.append(\"/Volumes/work/phd/2025/MoLFormer_fMRI/Graphormer\")\n",
    "# import graphormer\n",
    "\n",
    "from molfeat.trans.pretrained import GraphormerTransformer\n",
    "# import datamol as dm\n",
    "transformer = GraphormerTransformer(s='pcqm4mv2_graphormer_base', dtype=float)\n",
    "# smiles = dm.freesolv().iloc[:100].smiles\n",
    "\n",
    "# extract_representations_by_molfeat('pcqm4mv2_graphormer_base',transformer,input_type='smiles',token=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6676b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from molfeat.utils import requires\n",
    "print(requires.check(\"graphormer_pretrained\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aff3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 13:52:13 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f5381d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/graphormer38/lib/python3.8/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "print(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1aed812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/farzaneh/opt/anaconda3/envs/molfeat_graphormer/bin/python\n",
      "Python 3.11.13\n"
     ]
    }
   ],
   "source": [
    "# ! conda activate molfeat_graphormer\n",
    "! which python\n",
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad8601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoLFormer_fMRI",
   "language": "python",
   "name": "molformer_fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
