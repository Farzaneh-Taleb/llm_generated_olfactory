{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T11:33:43.289449Z",
     "start_time": "2025-03-30T11:33:43.259703Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer,AutoModelForMaskedLM, AutoModelForCausalLM,AutoModelForSeq2SeqLM,GraphormerForGraphClassification\n",
    "import pubchempy as pcp\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from rdkit import Chem\n",
    "import os\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from __future__ import annotations\n",
    "import os, torch\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, RDKFingerprint, MACCSkeys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8fa9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../../../../KINGSTON/DATASETS'\n",
    "base_dir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1495dca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mattention\u001b[m\u001b[m  \u001b[34mdatasets\u001b[m\u001b[m   \u001b[34membeddings\u001b[m\u001b[m \u001b[34mfolds\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls '../../../../KINGSTON/DATASETS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d78a68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_seeds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mset_seeds\u001b[49m(seed=\u001b[32m2024\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'set_seeds' is not defined"
     ]
    }
   ],
   "source": [
    "# set_seeds(seed=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "430dacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def extract_tokens(\n",
    "    tokenizer,\n",
    "    model,              # kept for API symmetry, but not actually used here\n",
    "    model_name: str,   \n",
    "    ds: str,\n",
    "    input_type: str = \"smiles\",   # \"smiles\" or \"selfies\"\n",
    "    save_path: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract *token IDs* from UNIQUE CIDs using either SMILES or SELFIES.\n",
    "\n",
    "    Output columns:\n",
    "        ['cid', 'isomeric_text', 'canonical_text',\n",
    "         'input_type', 'model', 'iso_token_ids', 'can_token_ids']\n",
    "\n",
    "    - iso_token_ids, can_token_ids are JSON-encoded lists of ints\n",
    "      corresponding to tokenizer(...)[\\\"input_ids\\\"].\n",
    "    \"\"\"\n",
    "\n",
    "    assert input_type.lower() in {\"smiles\", \"selfies\"}, \"input_type must be 'smiles' or 'selfies'\"\n",
    "    print(f\"Extracting {model_name} TOKENS for {ds} using {input_type}...\")\n",
    "\n",
    "    # ---- Load & pick columns ----\n",
    "    df = pd.read_csv(f\"{base_dir}/datasets/{ds}/{ds}_data.csv\")\n",
    "    if \"cid\" not in df.columns:\n",
    "        raise ValueError(\"Dataset must contain a 'cid' column.\")\n",
    "\n",
    "    if input_type.lower() == \"smiles\":\n",
    "        iso_col = \"isomericsmiles\"\n",
    "        can_col = \"canonicalsmiles\"\n",
    "    else:  # selfies\n",
    "        iso_col = \"isomericselfies\"\n",
    "        can_col = \"canonicalselfies\"\n",
    "\n",
    "    if iso_col is None and can_col is None:\n",
    "        raise ValueError(f\"No {input_type} columns found.\")\n",
    "\n",
    "    use_cols = [\"cid\"]\n",
    "    if iso_col: use_cols.append(iso_col)\n",
    "    if can_col: use_cols.append(can_col)\n",
    "\n",
    "    work = df[use_cols].copy()\n",
    "    work = (\n",
    "        work\n",
    "        .drop_duplicates(subset=[\"cid\"], keep=\"first\")\n",
    "        .sort_values(\"cid\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for idx, row_in in work.iterrows():\n",
    "        cid = row_in[\"cid\"]\n",
    "        iso_txt = row_in.get(iso_col, \"\")\n",
    "        can_txt = row_in.get(can_col, \"\")\n",
    "\n",
    "        print(f\"Processing CID {cid}...\")\n",
    "\n",
    "        # --- normalize texts ---\n",
    "        iso_txt = iso_txt if isinstance(iso_txt, str) and iso_txt.strip() else \"\"\n",
    "        can_txt = can_txt if isinstance(can_txt, str) and can_txt.strip() else \"\"\n",
    "        if not iso_txt and not can_txt:\n",
    "            # nothing to encode for this cid\n",
    "            continue\n",
    "\n",
    "        iso_ids = None\n",
    "        can_ids = None\n",
    "\n",
    "        # --- tokenize isomeric (if present) ---\n",
    "        if iso_txt:\n",
    "            iso_inputs = tokenizer(\n",
    "                [iso_txt],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            # (1, T) -> list of ints\n",
    "            iso_ids = iso_inputs[\"input_ids\"][0].tolist()\n",
    "\n",
    "        # --- tokenize canonical (if present) ---\n",
    "        if can_txt:\n",
    "            can_inputs = tokenizer(\n",
    "                [can_txt],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            can_ids = can_inputs[\"input_ids\"][0].tolist()\n",
    "\n",
    "        out_row = {\n",
    "            \"cid\": cid,\n",
    "            \"isomeric_text\": iso_txt,\n",
    "            \"canonical_text\": can_txt,\n",
    "            \"input_type\": input_type.lower(),   # \"smiles\" or \"selfies\"\n",
    "            \"model\": model_name,\n",
    "            # store lists as JSON strings so lengths can vary per molecule\n",
    "            \"iso_token_ids\": json.dumps(iso_ids) if iso_ids is not None else \"\",\n",
    "            \"can_token_ids\": json.dumps(can_ids) if can_ids is not None else \"\",\n",
    "        }\n",
    "\n",
    "        rows.append(out_row)\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "\n",
    "    if save_path is not None:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        out_file = f\"{save_path}/{ds}_{model_name.split('/')[1]}_tokens.csv\"\n",
    "        print(f\"Saving to {out_file}\")\n",
    "        out_df.to_csv(out_file, index=False)\n",
    "\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0ba022",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_representations\u001b[39m(\n\u001b[32m      2\u001b[39m     tokenizer,\n\u001b[32m      3\u001b[39m     model,\n\u001b[32m      4\u001b[39m     model_name: \u001b[38;5;28mstr\u001b[39m,   \n\u001b[32m      5\u001b[39m     ds: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m      6\u001b[39m     input_type: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33msmiles\u001b[39m\u001b[33m\"\u001b[39m,   \u001b[38;5;66;03m# \"smiles\" or \"selfies\"\u001b[39;00m\n\u001b[32m      7\u001b[39m     token: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,               \u001b[38;5;66;03m# which token to read (e.g., CLS/first)\u001b[39;00m\n\u001b[32m      8\u001b[39m     save_path: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      9\u001b[39m     mean : \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m ) -> \u001b[43mpd\u001b[49m.DataFrame:\n\u001b[32m     11\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m    Create embeddings from UNIQUE CIDs using either SMILES or SELFIES.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    The output DF has:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \u001b[33;03m    otherwise falls back to the canonical text. Both text variants are kept as metadata columns.\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m input_type.lower() \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33msmiles\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mselfies\u001b[39m\u001b[33m\"\u001b[39m}, \u001b[33m\"\u001b[39m\u001b[33minput_type must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33msmiles\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mselfies\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_representations(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    model_name: str,   \n",
    "    ds: str,\n",
    "    input_type: str = \"smiles\",   # \"smiles\" or \"selfies\"\n",
    "    token: int = 0,               # which token to read (e.g., CLS/first)\n",
    "    save_path: str | None = None,\n",
    "    mean : bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create embeddings from UNIQUE CIDs using either SMILES or SELFIES.\n",
    "    The output DF has:\n",
    "        ['cid', 'isomeric_text', 'canonical_text', 'input_type', 'model', 'layer', 'e0', ..., 'e{d-1}'].\n",
    "\n",
    "    Tokenization uses the chosen input_type; for each row it prefers the isomeric text if present,\n",
    "    otherwise falls back to the canonical text. Both text variants are kept as metadata columns.\n",
    "    \"\"\"\n",
    "    assert input_type.lower() in {\"smiles\", \"selfies\"}, \"input_type must be 'smiles' or 'selfies'\"\n",
    "    model.eval()\n",
    "    print(f\"Extracting {model_name} embeddings for {ds} using {input_type}...\")\n",
    "\n",
    "    # ---- Load & pick columns ----\n",
    "    df = pd.read_csv(f\"{base_dir}/datasets/{ds}/{ds}_data.csv\")\n",
    "    if \"cid\" not in df.columns:\n",
    "        raise ValueError(\"Dataset must contain a 'cid' column.\")\n",
    "\n",
    "    if input_type.lower() == \"smiles\":\n",
    "        iso_col = \"isomericsmiles\"\n",
    "        can_col = \"canonicalsmiles\"\n",
    "    else:  # selfies\n",
    "        iso_col = \"isomericselfies\"\n",
    "        can_col = \"canonicalselfies\"\n",
    "\n",
    "    if iso_col is None and can_col is None:\n",
    "        raise ValueError(f\"No {input_type} columns found.\")\n",
    "\n",
    "    use_cols = [\"cid\"]\n",
    "    if iso_col: use_cols.append(iso_col)\n",
    "    if can_col: use_cols.append(can_col)\n",
    "\n",
    "    work = df[use_cols].copy()\n",
    "    work = work.drop_duplicates(subset=[\"cid\"], keep=\"first\").sort_values(\"cid\").reset_index(drop=True)\n",
    "\n",
    "   \n",
    "    # ---- Device ----\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    rows = []\n",
    "    emb_dim = None\n",
    "\n",
    "    for cid, iso_txt, can_txt in zip(work[\"cid\"], work[iso_col], work[can_col]):\n",
    "        print(f\"Processing CID {cid}...\")\n",
    "        # --- normalize texts ---\n",
    "        iso_txt = iso_txt if isinstance(iso_txt, str) and iso_txt.strip() else \"\"\n",
    "        can_txt = can_txt if isinstance(can_txt, str) and can_txt.strip() else \"\"\n",
    "        if not iso_txt and not can_txt:\n",
    "            continue  # nothing to encode for this cid\n",
    "\n",
    "        # --- run model for isomeric (if present) ---\n",
    "        iso_hiddens = None\n",
    "        if iso_txt:\n",
    "            iso_inputs = tokenizer([iso_txt], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            iso_inputs = {k: v.to(device) for k, v in iso_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                iso_out = model(**iso_inputs, output_hidden_states=True)\n",
    "            iso_hiddens = iso_out.hidden_states  # tuple of [B,T,D] tensors\n",
    "\n",
    "        # --- run model for canonical (if present) ---\n",
    "        can_hiddens = None\n",
    "        if can_txt:\n",
    "            can_inputs = tokenizer([can_txt], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            can_inputs = {k: v.to(device) for k, v in can_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                can_out = model(**can_inputs, output_hidden_states=True)\n",
    "            can_hiddens = can_out.hidden_states\n",
    "\n",
    "        # number of layers to emit = max of the two (they should match)\n",
    "        n_layers = max(\n",
    "            len(iso_hiddens) if iso_hiddens is not None else 0,\n",
    "            len(can_hiddens) if can_hiddens is not None else 0,\n",
    "        )\n",
    "\n",
    "        for layer_idx in range(n_layers):\n",
    "            # get vectors (or None) for this layer\n",
    "            # print(f\"Processing CID {cid}, layer {layer_idx+1}/{n_layers}\")\n",
    "           \n",
    "            if mean:\n",
    "                iso_vec = iso_hiddens[layer_idx][0, :].mean(dim=0).detach().cpu().numpy()\n",
    "                can_vec = can_hiddens[layer_idx][0, :].mean(dim=0).detach().cpu().numpy()\n",
    "            else:\n",
    "                iso_vec = iso_hiddens[layer_idx][0, token, :].detach().cpu().numpy()\n",
    "                can_vec = can_hiddens[layer_idx][0, token, :].detach().cpu().numpy()    \n",
    "\n",
    "            # set / check embedding dim\n",
    "            if emb_dim is None:\n",
    "                \n",
    "                emb_dim = iso_vec.shape[0]\n",
    "                emb_dim = can_vec.shape[0]\n",
    "            # sanity: if both exist, ensure same D\n",
    "            if (iso_vec is not None) and (can_vec is not None):\n",
    "                assert iso_vec.shape[0] == can_vec.shape[0], \"Iso/Can dims differ!\"\n",
    "\n",
    "            row = {\n",
    "                \"cid\": cid,\n",
    "                \"isomeric_text\": iso_txt,\n",
    "                \"canonical_text\": can_txt,\n",
    "                \"input_type\": input_type.lower(),   # \"smiles\" or \"selfies\"\n",
    "                \"model\": model_name,\n",
    "                \"layer\": layer_idx,\n",
    "            }\n",
    "\n",
    "            # add iso_* columns (fill with NaN if missing)\n",
    "            if emb_dim is None:\n",
    "                continue  # defensive; should not happen if any vec exists\n",
    "            for i in range(emb_dim):\n",
    "                row[f\"iso_e{i}\"] = float(iso_vec[i]) \n",
    "                row[f\"can_e{i}\"] = float(can_vec[i])\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "\n",
    "        \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    print(f\"Saving to {save_path}/{ds}_{model_name.split('/')[1]}_embeddings.csv\")\n",
    "    out_df.to_csv(f\"{save_path}/{ds}_{model_name.split('/')[1]}_embeddings.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3cb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def extract_fingerprints(\n",
    "    ds: str,\n",
    "    fp_type: str = \"morgan\",      # \"morgan\", \"rdkit\", or \"maccs\"\n",
    "    radius: int = 2,              # for Morgan\n",
    "    n_bits: int = 1024,           # bit length for Morgan/RDKit\n",
    "    save_path: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create **SMILES-based fingerprints** from UNIQUE CIDs.\n",
    "\n",
    "    Assumes the dataset CSV (base_dir/datasets/{ds}/{ds}_data.csv) has:\n",
    "        - 'cid'\n",
    "        - 'isomericsmiles'\n",
    "        - 'canonicalsmiles'\n",
    "\n",
    "    Output columns:\n",
    "        ['cid', 'isomeric_text', 'canonical_text',\n",
    "         'input_type', 'model', 'layer',\n",
    "         'iso_e0', ..., 'iso_e{d-1}',\n",
    "         'can_e0', ..., 'can_e{d-1}']\n",
    "\n",
    "    - 'layer' is always 0 so this stays compatible with your embedding schema.\n",
    "    - e* columns are now binary fingerprint bits (0/1 or NaN if missing).\n",
    "    \"\"\"\n",
    "    fp_type = fp_type.lower()\n",
    "\n",
    "    \n",
    "    if fp_type == \"morgan\":\n",
    "        model_name = f\"Morgan_r{radius}_{n_bits}\"\n",
    "    elif fp_type == \"rdkit\":\n",
    "        model_name = f\"RDK_{n_bits}\"\n",
    "    elif fp_type == \"maccs\":\n",
    "        model_name = \"MACCS\"\n",
    "    else:\n",
    "        raise ValueError(\"fp_type must be one of 'morgan', 'rdkit', 'maccs'.\")\n",
    "    print(f\"Extracting {fp_type} fingerprints for dataset '{ds}' (SMILES only)...\")\n",
    "\n",
    "    # ---- Load & pick columns ----\n",
    "    df = pd.read_csv(f\"{base_dir}/datasets/{ds}/{ds}_data.csv\")\n",
    "\n",
    "    if \"cid\" not in df.columns:\n",
    "        raise ValueError(\"Dataset must contain a 'cid' column.\")\n",
    "    for col in (\"isomericsmiles\", \"canonicalsmiles\"):\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Dataset must contain column '{col}' for SMILES fingerprints.\")\n",
    "\n",
    "    iso_col = \"isomericsmiles\"\n",
    "    can_col = \"canonicalsmiles\"\n",
    "\n",
    "    work = df[[\"cid\", iso_col, can_col]].copy()\n",
    "    work = (\n",
    "        work.drop_duplicates(subset=[\"cid\"], keep=\"first\")\n",
    "            .sort_values(\"cid\")\n",
    "            .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    fp_dim = None  # number of bits (set on first successful FP)\n",
    "\n",
    "    # ---- helper: build fingerprint from SMILES ----\n",
    "    def make_fp(smiles: str):\n",
    "        nonlocal fp_dim\n",
    "\n",
    "        if not isinstance(smiles, str) or not smiles.strip():\n",
    "            return None\n",
    "        smiles = smiles.strip()\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "\n",
    "        if fp_type == \"morgan\":\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "        elif fp_type == \"rdkit\":\n",
    "            fp = RDKFingerprint(mol, fpSize=n_bits)\n",
    "        elif fp_type == \"maccs\":\n",
    "            fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "        else:\n",
    "            raise ValueError(\"fp_type must be one of 'morgan', 'rdkit', 'maccs'.\")\n",
    "\n",
    "        num_bits = fp.GetNumBits()\n",
    "        if fp_dim is None:\n",
    "            fp_dim = num_bits\n",
    "        else:\n",
    "            assert fp_dim == num_bits, \"Inconsistent fingerprint length across molecules.\"\n",
    "\n",
    "        arr = np.zeros((num_bits,), dtype=int)\n",
    "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        return arr\n",
    "\n",
    "    # ---- Main loop over CIDs ----\n",
    "    for cid, iso_txt, can_txt in zip(work[\"cid\"], work[iso_col], work[can_col]):\n",
    "        print(f\"Processing CID {cid}...\")\n",
    "\n",
    "        iso_fp = make_fp(iso_txt)\n",
    "        can_fp = make_fp(can_txt)\n",
    "\n",
    "        # skip if nothing could be computed\n",
    "        if iso_fp is None and can_fp is None:\n",
    "            continue\n",
    "\n",
    "        # ensure dims match if both exist\n",
    "        if iso_fp is not None and can_fp is not None:\n",
    "            assert iso_fp.shape[0] == can_fp.shape[0], \"Iso/Can fingerprint dims differ!\"\n",
    "\n",
    "        if fp_dim is None:\n",
    "            # defensive; shouldn't happen if any fp exists\n",
    "            continue\n",
    "\n",
    "        layer_idx = 0  # keep layer axis to match your embedding tables\n",
    "\n",
    "        row = {\n",
    "            \"cid\": cid,\n",
    "            \"isomeric_text\": iso_txt if isinstance(iso_txt, str) else \"\",\n",
    "            \"canonical_text\": can_txt if isinstance(can_txt, str) else \"\",\n",
    "            \"input_type\": \"smiles\",\n",
    "            \"model\": model_name,\n",
    "            \"layer\": layer_idx,\n",
    "        }\n",
    "\n",
    "        # Fill iso_e* and can_e* (bits or NaN)\n",
    "        for i in range(fp_dim):\n",
    "            if iso_fp is not None:\n",
    "                row[f\"iso_e{i}\"] = int(iso_fp[i])\n",
    "            else:\n",
    "                row[f\"iso_e{i}\"] = np.nan\n",
    "\n",
    "            if can_fp is not None:\n",
    "                row[f\"can_e{i}\"] = int(can_fp[i])\n",
    "            else:\n",
    "                row[f\"can_e{i}\"] = np.nan\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "\n",
    "    if save_path is not None:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        fname = f\"{save_path}/{ds}_{model_name}_embeddings.csv\"\n",
    "        print(f\"Saving to {fname}\")\n",
    "        out_df.to_csv(fname, index=False)\n",
    "\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "973ee3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rdkit_descriptors(\n",
    "    ds: str,\n",
    "    save_path: str | None = None,\n",
    "    model_name: str = \"RDKit_Descriptors\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract **all available RDKit molecular descriptors** for SMILES-only datasets.\n",
    "\n",
    "    Output columns:\n",
    "        ['cid', 'isomeric_text', 'canonical_text', 'input_type', 'model', 'layer',\n",
    "         'iso_DESCNAME1', ..., 'iso_DESCNAMEN',\n",
    "         'can_DESCNAME1', ..., 'can_DESCNAMEN']\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Extracting full RDKit descriptor set for dataset '{ds}' (SMILES only)...\")\n",
    "\n",
    "    # ---- Load dataset ----\n",
    "    df = pd.read_csv(f\"{base_dir}/datasets/{ds}/{ds}_data.csv\")\n",
    "\n",
    "    if \"cid\" not in df.columns:\n",
    "        raise ValueError(\"Dataset must contain a 'cid' column.\")\n",
    "\n",
    "    for col in (\"isomericsmiles\", \"canonicalsmiles\"):\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Dataset must contain column '{col}' for SMILES descriptors.\")\n",
    "\n",
    "    iso_col = \"isomericsmiles\"\n",
    "    can_col = \"canonicalsmiles\"\n",
    "\n",
    "    work = df[[\"cid\", iso_col, can_col]].drop_duplicates(\"cid\").sort_values(\"cid\").reset_index(drop=True)\n",
    "\n",
    "    # ---- Descriptor names ----\n",
    "    descriptor_names = [desc_name for desc_name, _ in Descriptors._descList]\n",
    "\n",
    "    calc = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # ---- Helper ----\n",
    "    def compute_descriptors(smiles):\n",
    "        \"\"\"Return list of descriptor values or None if molecule cannot be parsed.\"\"\"\n",
    "        if not isinstance(smiles, str) or not smiles.strip():\n",
    "            return None\n",
    "        mol = Chem.MolFromSmiles(smiles.strip())\n",
    "        if mol is None:\n",
    "            return None\n",
    "        try:\n",
    "            vals = calc.CalcDescriptors(mol)\n",
    "            return list(vals)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # ---- Main loop ----\n",
    "    for cid, iso_txt, can_txt in zip(work[\"cid\"], work[iso_col], work[can_col]):\n",
    "        print(f\"Processing CID {cid}...\")\n",
    "\n",
    "        iso_desc = compute_descriptors(iso_txt)\n",
    "        can_desc = compute_descriptors(can_txt)\n",
    "\n",
    "        # skip if no descriptors could be computed\n",
    "        if iso_desc is None and can_desc is None:\n",
    "            continue\n",
    "\n",
    "        row = {\n",
    "            \"cid\": cid,\n",
    "            \"isomeric_text\": iso_txt if isinstance(iso_txt, str) else \"\",\n",
    "            \"canonical_text\": can_txt if isinstance(can_txt, str) else \"\",\n",
    "            \"input_type\": \"smiles\",\n",
    "            \"model\": model_name,\n",
    "            \"layer\": 0,  # to match your existing schema\n",
    "        }\n",
    "\n",
    "        # Fill descriptor columns\n",
    "        for name_idx, desc_name in enumerate(descriptor_names):\n",
    "            iso_val = iso_desc[name_idx] if iso_desc is not None else np.nan\n",
    "            can_val = can_desc[name_idx] if can_desc is not None else np.nan\n",
    "\n",
    "            row[f\"iso_{desc_name}\"] = iso_val\n",
    "            row[f\"can_{desc_name}\"] = can_val\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "\n",
    "    # ---- Save ----\n",
    "    if save_path is not None:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        fname = f\"{save_path}/{ds}_{model_name}.csv\"\n",
    "        print(f\"Saving RDKit descriptor table to: {fname}\")\n",
    "        out_df.to_csv(fname, index=False)\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c09799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_attention_weights(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    model_name: str,\n",
    "    ds: str,\n",
    "    input_type: str = \"smiles\",  # \"smiles\" or \"selfies\"\n",
    "    token: int = 0,              # which query token index to use (e.g. CLS at 0)\n",
    "    save_path: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract attention weights from UNIQUE CIDs using either SMILES or SELFIES.\n",
    "\n",
    "    For each CID and each layer, we store:\n",
    "        - head-averaged attention distribution from `token` -> all tokens\n",
    "        - separately for isomeric and canonical text (if available)\n",
    "\n",
    "    Output DF columns:\n",
    "        ['cid', 'isomeric_text', 'canonical_text', 'input_type', 'model', 'layer',\n",
    "         'iso_att', 'can_att']\n",
    "\n",
    "    where 'iso_att' and 'can_att' are Python lists (variable length, depends on seq len).\n",
    "    \"\"\"\n",
    "    assert input_type.lower() in {\"smiles\", \"selfies\"}, \"input_type must be 'smiles' or 'selfies'\"\n",
    "    model.eval()\n",
    "    print(f\"Extracting {model_name} ATTENTIONS for {ds} using {input_type}...\")\n",
    "\n",
    "    # ---- Load & pick columns ----\n",
    "    df = pd.read_csv(f\"{base_dir}/datasets/{ds}/{ds}_data.csv\")\n",
    "    if \"cid\" not in df.columns:\n",
    "        raise ValueError(\"Dataset must contain a 'cid' column.\")\n",
    "\n",
    "    if input_type.lower() == \"smiles\":\n",
    "        iso_col = \"isomericsmiles\"\n",
    "        can_col = \"canonicalsmiles\"\n",
    "    else:\n",
    "        iso_col = \"isomericselfies\"\n",
    "        can_col = \"canonicalselfies\"\n",
    "\n",
    "    use_cols = [\"cid\"]\n",
    "    if iso_col in df.columns:\n",
    "        use_cols.append(iso_col)\n",
    "    if can_col in df.columns:\n",
    "        use_cols.append(can_col)\n",
    "\n",
    "    work = (\n",
    "        df[use_cols]\n",
    "        .drop_duplicates(subset=[\"cid\"], keep=\"first\")\n",
    "        .sort_values(\"cid\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # ---- Device ----\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for _, row_df in work.iterrows():\n",
    "        cid = row_df[\"cid\"]\n",
    "        iso_txt = row_df.get(iso_col, \"\")\n",
    "        can_txt = row_df.get(can_col, \"\")\n",
    "\n",
    "        print(f\"Processing CID {cid}...\")\n",
    "\n",
    "        # normalize texts\n",
    "        iso_txt = iso_txt if isinstance(iso_txt, str) and iso_txt.strip() else \"\"\n",
    "        can_txt = can_txt if isinstance(can_txt, str) and can_txt.strip() else \"\"\n",
    "        if not iso_txt and not can_txt:\n",
    "            continue\n",
    "\n",
    "        # --- run model for isomeric (if present) ---\n",
    "        iso_atts = None\n",
    "        if iso_txt:\n",
    "            iso_inputs = tokenizer([iso_txt], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            iso_inputs = {k: v.to(device) for k, v in iso_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                iso_out = model(**iso_inputs, output_attentions=True)\n",
    "            iso_atts = iso_out.attentions  # tuple of length n_layers, each (B, H, T, T)\n",
    "\n",
    "        # --- run model for canonical (if present) ---\n",
    "        can_atts = None\n",
    "        if can_txt:\n",
    "            can_inputs = tokenizer([can_txt], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            can_inputs = {k: v.to(device) for k, v in can_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                can_out = model(**can_inputs, output_attentions=True)\n",
    "            can_atts = can_out.attentions\n",
    "\n",
    "        n_layers = max(\n",
    "            len(iso_atts) if iso_atts is not None else 0,\n",
    "            len(can_atts) if can_atts is not None else 0,\n",
    "        )\n",
    "\n",
    "        for layer_idx in range(n_layers):\n",
    "            iso_att_vec = None\n",
    "            can_att_vec = None\n",
    "\n",
    "            # ---- isomeric attention: CLS/token -> all tokens, averaged over heads ----\n",
    "            if iso_atts is not None:\n",
    "                att = iso_atts[layer_idx]  # (1, H, T, T)\n",
    "                # make sure token index is valid\n",
    "                T = att.shape[-1]\n",
    "                q_idx = token if token < T else 0\n",
    "                # average over heads -> (T, T), then pick row for q_idx -> (T,)\n",
    "                att_avg = att[0].mean(dim=0)           # (T, T)\n",
    "                iso_att_vec = att_avg[q_idx].detach().cpu().numpy()  # (T,)\n",
    "\n",
    "            # ---- canonical attention: CLS/token -> all tokens, averaged over heads ----\n",
    "            if can_atts is not None:\n",
    "                att = can_atts[layer_idx]  # (1, H, T, T)\n",
    "                T = att.shape[-1]\n",
    "                q_idx = token if token < T else 0\n",
    "                att_avg = att[0].mean(dim=0)           # (T, T)\n",
    "                can_att_vec = att_avg[q_idx].detach().cpu().numpy()  # (T,)\n",
    "\n",
    "            row = {\n",
    "                \"cid\": cid,\n",
    "                \"isomeric_text\": iso_txt,\n",
    "                \"canonical_text\": can_txt,\n",
    "                \"input_type\": input_type.lower(),\n",
    "                \"model\": model_name,\n",
    "                \"layer\": layer_idx,\n",
    "                # store as lists; seq len can differ per CID\n",
    "                \"iso_att\": iso_att_vec.tolist() if iso_att_vec is not None else None,\n",
    "                \"can_att\": can_att_vec.tolist() if can_att_vec is not None else None,\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "\n",
    "    if save_path is not None:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        out_file = f\"{save_path}/{ds}_{model_name.split('/')[-1]}_attentions.csv\"\n",
    "        print(f\"Saving to {out_file}\")\n",
    "        out_df.to_csv(out_file, index=False)\n",
    "\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def extract_representations_pairs(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    model_name: str,\n",
    "    ds: str,\n",
    "    input_type: str = \"smiles\",   # \"smiles\" or \"selfies\"\n",
    "    token: int = 0,               # CLS/first token index\n",
    "    save_path: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract per-layer embeddings for BOTH stimuli in a pairwise similarity dataset.\n",
    "\n",
    "    Expected columns:\n",
    "        cid stimulus 1, cid stimulus 2, similarity,\n",
    "        isomericsmiles stimulus 1, isomericsmiles stimulus 2,\n",
    "        canonicalsmiles stimulus 1, canonicalsmiles stimulus 2,\n",
    "        isomericselfies stimulus 1, isomericselfies stimulus 2,\n",
    "        canonicalselfies stimulus 1, canonicalselfies stimulus 2\n",
    "\n",
    "    Output columns:\n",
    "        ['pair_index','stimulus_id','cid','stimulus','isomeric_text','canonical_text',\n",
    "         'input_type','model','layer','e0','e1',...,'e{d-1}']\n",
    "    \"\"\"\n",
    "    assert input_type.lower() in {\"smiles\", \"selfies\"}, \"input_type must be 'smiles' or 'selfies'\"\n",
    "    model.eval()\n",
    "    print(f\"Extracting {model_name} embeddings for {ds} (pairwise mode, {input_type})...\")\n",
    "\n",
    "    # ---- Load ----\n",
    "    df = pd.read_csv(f\"../datasets/{ds}/{ds}_data.csv\")\n",
    "    required_cols = [f\"cid stimulus 1\", f\"cid stimulus 2\"]\n",
    "    if not all(c in df.columns for c in required_cols):\n",
    "        raise ValueError(f\"File must include: {required_cols}\")\n",
    "\n",
    "    # ---- Column mapping ----\n",
    "    if input_type.lower() == \"smiles\":\n",
    "        iso1_col, iso2_col = \"isomericsmiles stimulus 1\", \"isomericsmiles stimulus 2\"\n",
    "        can1_col, can2_col = \"canonicalsmiles stimulus 1\", \"canonicalsmiles stimulus 2\"\n",
    "    else:\n",
    "        iso1_col, iso2_col = \"isomericselfies stimulus 1\", \"isomericselfies stimulus 2\"\n",
    "        can1_col, can2_col = \"canonicalselfies stimulus 1\", \"canonicalselfies stimulus 2\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    rows = []\n",
    "    emb_dim = None\n",
    "\n",
    "    # iterate through each pair\n",
    "    for i, row in df.iterrows():\n",
    "        for stim_idx in [1, 2]:\n",
    "            cid = row[f\"cid stimulus {stim_idx}\"]\n",
    "            iso_txt = row.get(f\"{'isomeric' + input_type} stimulus {stim_idx}\", None)\n",
    "            can_txt = row.get(f\"canonical{input_type} stimulus {stim_idx}\", None)\n",
    "\n",
    "            iso_txt = str(iso_txt).strip() if isinstance(iso_txt, str) else \"\"\n",
    "            can_txt = str(can_txt).strip() if isinstance(can_txt, str) else \"\"\n",
    "            if not iso_txt and not can_txt:\n",
    "                continue\n",
    "\n",
    "            # tokenize and run model for both iso and can if available\n",
    "            def run_encoder(txt: str):\n",
    "                if not txt: \n",
    "                    return None\n",
    "                inputs = tokenizer([txt], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "                out = model(**inputs, output_hidden_states=True)\n",
    "                return out.hidden_states  # list of [B, T, D]\n",
    "\n",
    "            iso_hiddens = run_encoder(iso_txt)\n",
    "            can_hiddens = run_encoder(can_txt)\n",
    "\n",
    "            n_layers = max(\n",
    "                len(iso_hiddens) if iso_hiddens is not None else 0,\n",
    "                len(can_hiddens) if can_hiddens is not None else 0,\n",
    "            )\n",
    "\n",
    "            for layer_idx in range(n_layers):\n",
    "                iso_vec = iso_hiddens[layer_idx][0, token, :].detach().cpu().numpy() if iso_hiddens else None\n",
    "                can_vec = can_hiddens[layer_idx][0, token, :].detach().cpu().numpy() if can_hiddens else None\n",
    "\n",
    "                if emb_dim is None:\n",
    "                    emb_dim = iso_vec.shape[0] if iso_vec is not None else can_vec.shape[0]\n",
    "\n",
    "                row_out = {\n",
    "                    \"pair_index\": i,\n",
    "                    \"stimulus\": stim_idx,\n",
    "                    \"cid\": cid,\n",
    "                    \"isomeric_text\": iso_txt,\n",
    "                    \"canonical_text\": can_txt,\n",
    "                    \"input_type\": input_type.lower(),\n",
    "                    \"model\": model_name,\n",
    "                    \"layer\": layer_idx,\n",
    "                }\n",
    "\n",
    "                # fill embedding vector\n",
    "                if iso_vec is not None:\n",
    "                    for j in range(emb_dim):\n",
    "                        row_out[f\"iso_e{j}\"] = float(iso_vec[j])\n",
    "                if can_vec is not None:\n",
    "                    for j in range(emb_dim):\n",
    "                        row_out[f\"can_e{j}\"] = float(can_vec[j])\n",
    "\n",
    "                rows.append(row_out)\n",
    "\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Processed {i+1}/{len(df)} pairs...\")\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    out_file = f\"{save_path}/{ds}_{model_name.split('/')[-1]}_pair_embeddings.csv\"\n",
    "    out_df.to_csv(out_file, index=False)\n",
    "    print(f\"Saved embeddings â†’ {out_file}\")\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57701c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_representations_per_token(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    model_name: str,\n",
    "    ds: str,\n",
    "    input_type: str = \"smiles\",   # \"smiles\" or \"selfies\"\n",
    "    save_path: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each CID, layer, and token:\n",
    "        -> one row with the embedding of that token at that layer.\n",
    "\n",
    "    Columns:\n",
    "        - cid\n",
    "        - isomeric_text\n",
    "        - canonical_text\n",
    "        - used_text          (the string that was actually tokenized)\n",
    "        - used_variant       (\"iso\" or \"can\")\n",
    "        - input_type         (\"smiles\" / \"selfies\")\n",
    "        - model              (model_name)\n",
    "        - layer              (int, layer index as returned by model)\n",
    "        - token_idx          (position in the sequence)\n",
    "        - token_id           (vocab ID)\n",
    "        - token_str          (token text from tokenizer)\n",
    "        - e0 ... e{d-1}      (embedding dimensions)\n",
    "    \"\"\"\n",
    "\n",
    "    assert input_type.lower() in {\"smiles\", \"selfies\"}, \"input_type must be 'smiles' or 'selfies'\"\n",
    "    print(f\"Extracting {model_name} PER-TOKEN embeddings for {ds} using {input_type}...\")\n",
    "\n",
    "    # ---- Load & pick columns ----\n",
    "    df = pd.read_csv(f\"{base_dir}/datasets/{ds}/{ds}_data.csv\")\n",
    "    if \"cid\" not in df.columns:\n",
    "        raise ValueError(\"Dataset must contain a 'cid' column.\")\n",
    "\n",
    "    if input_type.lower() == \"smiles\":\n",
    "        iso_col = \"isomericsmiles\"\n",
    "        can_col = \"canonicalsmiles\"\n",
    "    else:  # selfies\n",
    "        iso_col = \"isomericselfies\"\n",
    "        can_col = \"canonicalselfies\"\n",
    "\n",
    "    if iso_col is None and can_col is None:\n",
    "        raise ValueError(f\"No {input_type} columns found.\")\n",
    "\n",
    "    use_cols = [\"cid\"]\n",
    "    if iso_col: use_cols.append(iso_col)\n",
    "    if can_col: use_cols.append(can_col)\n",
    "\n",
    "    work = (\n",
    "        df[use_cols]\n",
    "        .drop_duplicates(subset=[\"cid\"], keep=\"first\")\n",
    "        .sort_values(\"cid\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # ---- Device ----\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    rows = []\n",
    "    emb_dim = None\n",
    "\n",
    "    for _, row_in in work.iterrows():\n",
    "        cid = row_in[\"cid\"]\n",
    "        iso_txt = row_in.get(iso_col, \"\")\n",
    "        can_txt = row_in.get(can_col, \"\")\n",
    "\n",
    "        print(f\"Processing CID {cid}...\")\n",
    "\n",
    "        # normalize texts\n",
    "        iso_txt = iso_txt if isinstance(iso_txt, str) and iso_txt.strip() else \"\"\n",
    "        can_txt = can_txt if isinstance(can_txt, str) and can_txt.strip() else \"\"\n",
    "\n",
    "        # choose which text to encode: prefer isomeric, fallback to canonical\n",
    "        if iso_txt:\n",
    "            used_text = iso_txt\n",
    "            used_variant = \"iso\"\n",
    "        elif can_txt:\n",
    "            used_text = can_txt\n",
    "            used_variant = \"can\"\n",
    "        else:\n",
    "            # nothing to encode\n",
    "            continue\n",
    "\n",
    "        # ---- Tokenize & run model ----\n",
    "        inputs = tokenizer(\n",
    "            [used_text],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        hidden_states = out.hidden_states  # tuple of length n_layers, each (1, T, D)\n",
    "        input_ids = inputs[\"input_ids\"][0]  # (T,)\n",
    "        T = input_ids.shape[0]\n",
    "        n_layers = len(hidden_states)\n",
    "\n",
    "        # Set emb_dim once\n",
    "        if emb_dim is None:\n",
    "            emb_dim = hidden_states[0].shape[-1]\n",
    "\n",
    "        # ---- Loop over layers and tokens ----\n",
    "        for layer_idx in range(n_layers):\n",
    "            h = hidden_states[layer_idx][0]  # (T, D)\n",
    "\n",
    "            for token_idx in range(T):\n",
    "                token_id = int(input_ids[token_idx].item())\n",
    "                token_str = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "\n",
    "                vec = h[token_idx].detach().cpu().numpy()  # (D,)\n",
    "\n",
    "                row = {\n",
    "                    \"cid\": cid,\n",
    "                    \"isomeric_text\": iso_txt,\n",
    "                    \"canonical_text\": can_txt,\n",
    "                    \"used_text\": used_text,\n",
    "                    \"used_variant\": used_variant,  # \"iso\" / \"can\"\n",
    "                    \"input_type\": input_type.lower(),\n",
    "                    \"model\": model_name,\n",
    "                    \"layer\": layer_idx,\n",
    "                    \"token_idx\": token_idx,\n",
    "                    \"token_id\": token_id,\n",
    "                    \"token_str\": token_str,\n",
    "                }\n",
    "\n",
    "                # add embedding dims\n",
    "                for i in range(emb_dim):\n",
    "                    row[f\"e{i}\"] = float(vec[i])\n",
    "\n",
    "                rows.append(row)\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "\n",
    "    if save_path is not None:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        out_file = f\"{save_path}/{ds}_{model_name.split('/')[1]}_per_token_embeddings.csv\"\n",
    "        print(f\"Saving to {out_file}\")\n",
    "        out_df.to_csv(out_file, index=False)\n",
    "\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d15ab8fabf3601a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T07:32:48.485874Z",
     "start_time": "2025-03-31T07:32:48.439231Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Literal, Tuple\n",
    "\n",
    "def build_cosine_similarity_from_embeddings(ds,model_name,\n",
    "    *,\n",
    "    layer: Literal[\"last\", \"mean\"] | int = \"last\",\n",
    "    variant: Literal[\"iso\", \"can\", \"prefer_iso\", \"prefer_can\"] = \"prefer_iso\",\n",
    "    out_csv: Optional[str] = None,\n",
    "    overwrite_similarity: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between stimulus 1 and 2 using precomputed embeddings\n",
    "    from `extract_representations_pairs` and rebuild the pairwise CSV.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings_csv : str\n",
    "        CSV produced by extract_representations_pairs (with columns:\n",
    "        pair_index, stimulus, layer, iso_e*, can_e*, ...).\n",
    "    original_pairs_csv : str\n",
    "        Your original pairwise file (cid stimulus 1/2, names, SMILES/SELFIES, ...).\n",
    "    layer : {\"last\",\"mean\"} or int, default \"last\"\n",
    "        - \"last\": use max available layer per (pair_index, stimulus).\n",
    "        - \"mean\": average vectors across all available layers.\n",
    "        - int: use that specific layer index.\n",
    "    variant : {\"iso\",\"can\",\"prefer_iso\",\"prefer_can\"}, default \"prefer_iso\"\n",
    "        Which embedding to use. \"prefer_*\" falls back to the other if missing.\n",
    "    out_csv : Optional[str]\n",
    "        Where to save the result. If None, writes next to `original_pairs_csv`\n",
    "        with suffix `_cosine_from_embs.csv`.\n",
    "    overwrite_similarity : bool, default True\n",
    "        If True, replaces column `similarity`. If False, writes `similarity_cosine`.\n",
    "    verbose : bool, default True\n",
    "        Print progress.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The rebuilt DataFrame with cosine similarities written to disk.\n",
    "    \"\"\"\n",
    "    # ---- Load files ----\n",
    "    original_pairs_csv=f\"../datasets/{ds}/{ds}_data.csv\"\n",
    "    embeddings_csv=f\"embeddings/{ds}_{model_name.split('/')[1]}_pair_embeddings.csv\"\n",
    "    embs = pd.read_csv(embeddings_csv)\n",
    "    pairs = pd.read_csv(original_pairs_csv)\n",
    "\n",
    "    # Basic checks\n",
    "    for col in [\"pair_index\", \"stimulus\", \"layer\"]:\n",
    "        if col not in embs.columns:\n",
    "            raise ValueError(f\"Embeddings CSV must contain '{col}' (did it come from extract_representations_pairs?).\")\n",
    "\n",
    "    # Identify embedding columns present\n",
    "    iso_cols = [c for c in embs.columns if c.startswith(\"iso_e\")]\n",
    "    can_cols = [c for c in embs.columns if c.startswith(\"can_e\")]\n",
    "    if not iso_cols and not can_cols:\n",
    "        raise ValueError(\"Embeddings CSV has no 'iso_e*' nor 'can_e*' columns.\")\n",
    "\n",
    "    # Helper: extract a vector for a given (pair_index, stimulus)\n",
    "    def _subset_for(pid: int, stim: int) -> pd.DataFrame:\n",
    "        return embs[(embs[\"pair_index\"] == pid) & (embs[\"stimulus\"] == stim)]\n",
    "\n",
    "    def _vec_from_rows(rows: pd.DataFrame) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "        if rows.empty:\n",
    "            return None, None\n",
    "\n",
    "        # choose rows by layer\n",
    "        if layer == \"last\":\n",
    "            lyr = rows[\"layer\"].max()\n",
    "            rows = rows[rows[\"layer\"] == lyr]\n",
    "        elif layer == \"mean\":\n",
    "            pass  # keep all layers\n",
    "        else:\n",
    "            lyr = int(layer)\n",
    "            rows = rows[rows[\"layer\"] == lyr]\n",
    "            if rows.empty:\n",
    "                return None, None\n",
    "\n",
    "        v_iso = None\n",
    "        v_can = None\n",
    "\n",
    "        if iso_cols:\n",
    "            arr_iso = rows[iso_cols].to_numpy(dtype=float)  # (L_sel, D) or (1, D)\n",
    "            if not np.isnan(arr_iso).all():\n",
    "                if layer == \"mean\":\n",
    "                    v_iso = np.nanmean(arr_iso, axis=0)\n",
    "                else:\n",
    "                    v_iso = arr_iso[0]\n",
    "                if np.isnan(v_iso).all():\n",
    "                    v_iso = None\n",
    "\n",
    "        if can_cols:\n",
    "            arr_can = rows[can_cols].to_numpy(dtype=float)\n",
    "            if not np.isnan(arr_can).all():\n",
    "                if layer == \"mean\":\n",
    "                    v_can = np.nanmean(arr_can, axis=0)\n",
    "                else:\n",
    "                    v_can = arr_can[0]\n",
    "                if np.isnan(v_can).all():\n",
    "                    v_can = None\n",
    "\n",
    "        return v_iso, v_can\n",
    "\n",
    "    def _choose_variant(v_iso: Optional[np.ndarray], v_can: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "        if variant == \"iso\":\n",
    "            return v_iso\n",
    "        if variant == \"can\":\n",
    "            return v_can\n",
    "        if variant == \"prefer_iso\":\n",
    "            return v_iso if v_iso is not None else v_can\n",
    "        if variant == \"prefer_can\":\n",
    "            return v_can if v_can is not None else v_iso\n",
    "        return None\n",
    "\n",
    "    def _cosine(a: Optional[np.ndarray], b: Optional[np.ndarray]) -> float:\n",
    "        if a is None or b is None:\n",
    "            return float(\"nan\")\n",
    "        na = np.linalg.norm(a)\n",
    "        nb = np.linalg.norm(b)\n",
    "        if na == 0.0 or nb == 0.0 or not np.isfinite(na) or not np.isfinite(nb):\n",
    "            return float(\"nan\")\n",
    "        return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "    # ---- Compute per-pair cosine ----\n",
    "    sims = []\n",
    "    n = len(pairs)\n",
    "    for i, _ in pairs.iterrows():\n",
    "        rows1 = _subset_for(i, 1)\n",
    "        rows2 = _subset_for(i, 2)\n",
    "        v1_iso, v1_can = _vec_from_rows(rows1)\n",
    "        v2_iso, v2_can = _vec_from_rows(rows2)\n",
    "\n",
    "        v1 = _choose_variant(v1_iso, v1_can)\n",
    "        v2 = _choose_variant(v2_iso, v2_can)\n",
    "        sim = _cosine(v1, v2)\n",
    "        sims.append(sim)\n",
    "\n",
    "        if verbose and (i + 1) % 50 == 0:\n",
    "            print(f\"Processed {i+1}/{n} pairs...\")\n",
    "\n",
    "    out = pairs.copy()\n",
    "    target_col = \"similarity\" if overwrite_similarity else \"similarity_cosine\"\n",
    "    out[target_col] = sims\n",
    "\n",
    "    # ---- Save ----\n",
    "    if out_csv is None:\n",
    "        base, ext = os.path.splitext(original_pairs_csv)\n",
    "        out_csv = f\"{base}_{model_name.split('/')[1]}_cosine_from_embs_{variant}.csv\"\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    if verbose:\n",
    "        print(f\"Saved cosine-similarity file â†’ {out_csv}\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca77193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_representations_by_molfeat(model_name,transformer,input_type='smiles',token=0):\n",
    "    \n",
    "#     for subject_id in range(s_start,s_end+1):\n",
    "#         input_molecules = pd.read_csv(f'{base_dir}/fmri/embeddings{ds}/CIDs_smiles_selfies_{subject_id}{ds}.csv')[input_type].values.tolist()\n",
    "        \n",
    "#         outputs = transformer(input_molecules)\n",
    "#         np.save(f'{base_dir}/fmri/embeddings{ds}/embeddings_{model_name}_{subject_id}_{-1}{ds}.npy',outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e1f67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e49e21f0d63ede",
   "metadata": {},
   "source": [
    "# Encoder-Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5b65a",
   "metadata": {},
   "source": [
    "### Token/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "866e713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jonghyunlee/ChemBERT_ChEMBL_pretrained and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at HUBioDataLab/SELFormer and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "Input_types = {\n",
    "    'ibm/MoLFormer-XL-both-10pct':'smiles',\n",
    "    'seyonec/ChemBERTa-zinc-base-v1':'smiles',\n",
    "    'jonghyunlee/ChemBERT_ChEMBL_pretrained':'smiles',\n",
    "    \"HUBioDataLab/SELFormer\":'selfies'\n",
    "    }\n",
    "\n",
    "for model_name in ['ibm/MoLFormer-XL-both-10pct',\n",
    "                   'seyonec/ChemBERTa-zinc-base-v1',\"jonghyunlee/ChemBERT_ChEMBL_pretrained\",\"HUBioDataLab/SELFormer\"\n",
    "                   ]:\n",
    "    for ds in [\n",
    "        'sagar2023'\n",
    "        # ,'keller2016',\n",
    "        # 'bierling2025',\n",
    "            #    'leffingwell'\n",
    "               ]:\n",
    "    \n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True,use_safetensors=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        # extract_representations(tokenizer, model,model_name,save_path='avgtokenembeddings',ds=ds,input_type=Input_types[model_name],mean=True)\n",
    "        vocab = tokenizer.get_vocab()\n",
    "\n",
    "        # convert to id â†’ token\n",
    "        id_to_token = {id_: tok for tok, id_ in vocab.items()}\n",
    "\n",
    "        # make DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            \"token_id\": list(id_to_token.keys()),\n",
    "            \"token\": list(id_to_token.values())\n",
    "        })\n",
    "\n",
    "        # sort by token_id\n",
    "        df = df.sort_values(\"token_id\").reset_index(drop=True)\n",
    "\n",
    "        # save CSV\n",
    "        fname = f\"tokens/{model_name.split('/')[-1]}_vocab.csv\"\n",
    "        df.to_csv(fname, index=False)\n",
    "        # extract_tokens(tokenizer, model,model_name,save_path='tokens',ds=ds,input_type=Input_types[model_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c357405615704",
   "metadata": {},
   "source": [
    "## MoLFormer-XL-both-10pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9682defb4c9030c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T21:03:57.047530Z",
     "start_time": "2025-01-06T21:00:03.803379Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ibm/MoLFormer-XL-both-10pct PER-TOKEN embeddings for sagar2023 using smiles...\n",
      "Processing CID 126...\n",
      "Processing CID 177...\n",
      "Processing CID 196...\n",
      "Processing CID 239...\n",
      "Processing CID 240...\n",
      "Processing CID 261...\n",
      "Processing CID 263...\n",
      "Processing CID 325...\n",
      "Processing CID 326...\n",
      "Processing CID 356...\n",
      "Processing CID 379...\n",
      "Processing CID 460...\n",
      "Processing CID 650...\n",
      "Processing CID 660...\n",
      "Processing CID 957...\n",
      "Processing CID 1001...\n",
      "Processing CID 1032...\n",
      "Processing CID 1049...\n",
      "Processing CID 1068...\n",
      "Processing CID 1110...\n",
      "Processing CID 1136...\n",
      "Processing CID 2214...\n",
      "Processing CID 2345...\n",
      "Processing CID 2346...\n",
      "Processing CID 2758...\n",
      "Processing CID 2879...\n",
      "Processing CID 2969...\n",
      "Processing CID 3893...\n",
      "Processing CID 4133...\n",
      "Processing CID 5541...\n",
      "Processing CID 5610...\n",
      "Processing CID 5779...\n",
      "Processing CID 5780...\n",
      "Processing CID 5960...\n",
      "Processing CID 6050...\n",
      "Processing CID 6054...\n",
      "Processing CID 6106...\n",
      "Processing CID 6184...\n",
      "Processing CID 6448...\n",
      "Processing CID 6501...\n",
      "Processing CID 6549...\n",
      "Processing CID 6584...\n",
      "Processing CID 6590...\n",
      "Processing CID 6658...\n",
      "Processing CID 6943...\n",
      "Processing CID 6997...\n",
      "Processing CID 6998...\n",
      "Processing CID 7059...\n",
      "Processing CID 7095...\n",
      "Processing CID 7122...\n",
      "Processing CID 7136...\n",
      "Processing CID 7144...\n",
      "Processing CID 7151...\n",
      "Processing CID 7165...\n",
      "Processing CID 7335...\n",
      "Processing CID 7361...\n",
      "Processing CID 7410...\n",
      "Processing CID 7463...\n",
      "Processing CID 7500...\n",
      "Processing CID 7519...\n",
      "Processing CID 7593...\n",
      "Processing CID 7635...\n",
      "Processing CID 7654...\n",
      "Processing CID 7720...\n",
      "Processing CID 7731...\n",
      "Processing CID 7749...\n",
      "Processing CID 7761...\n",
      "Processing CID 7762...\n",
      "Processing CID 7795...\n",
      "Processing CID 7799...\n",
      "Processing CID 7803...\n",
      "Processing CID 7824...\n",
      "Processing CID 7937...\n",
      "Processing CID 7967...\n",
      "Processing CID 7983...\n",
      "Processing CID 7991...\n",
      "Processing CID 8077...\n",
      "Processing CID 8082...\n",
      "Processing CID 8093...\n",
      "Processing CID 8103...\n",
      "Processing CID 8118...\n",
      "Processing CID 8129...\n",
      "Processing CID 8174...\n",
      "Processing CID 8184...\n",
      "Processing CID 8186...\n",
      "Processing CID 8193...\n",
      "Processing CID 8375...\n",
      "Processing CID 8456...\n",
      "Processing CID 8658...\n",
      "Processing CID 8697...\n",
      "Processing CID 8712...\n",
      "Processing CID 8723...\n",
      "Processing CID 8785...\n",
      "Processing CID 8797...\n",
      "Processing CID 8857...\n",
      "Processing CID 8892...\n",
      "Processing CID 8908...\n",
      "Processing CID 8914...\n",
      "Processing CID 8918...\n",
      "Processing CID 9025...\n",
      "Processing CID 9589...\n",
      "Processing CID 9609...\n",
      "Processing CID 9862...\n",
      "Processing CID 10364...\n",
      "Processing CID 10400...\n",
      "Processing CID 10430...\n",
      "Processing CID 10448...\n",
      "Processing CID 10882...\n",
      "Processing CID 10890...\n",
      "Processing CID 10895...\n",
      "Processing CID 10925...\n",
      "Processing CID 11086...\n",
      "Processing CID 11428...\n",
      "Processing CID 11525...\n",
      "Processing CID 11527...\n",
      "Processing CID 11529...\n",
      "Processing CID 11569...\n",
      "Processing CID 11583...\n",
      "Processing CID 11614...\n",
      "Processing CID 11902...\n",
      "Processing CID 11980...\n",
      "Processing CID 12180...\n",
      "Processing CID 12297...\n",
      "Processing CID 12327...\n",
      "Processing CID 12587...\n",
      "Processing CID 12741...\n",
      "Processing CID 12810...\n",
      "Processing CID 12813...\n",
      "Processing CID 14257...\n",
      "Processing CID 14286...\n",
      "Processing CID 14296...\n",
      "Processing CID 14491...\n",
      "Processing CID 14514...\n",
      "Processing CID 14525...\n",
      "Processing CID 15037...\n",
      "Processing CID 15380...\n",
      "Processing CID 15510...\n",
      "Processing CID 16741...\n",
      "Processing CID 17525...\n",
      "Processing CID 17617...\n",
      "Processing CID 18635...\n",
      "Processing CID 18827...\n",
      "Processing CID 21057...\n",
      "Processing CID 21648...\n",
      "Processing CID 22873...\n",
      "Processing CID 23235...\n",
      "Processing CID 23642...\n",
      "Processing CID 24834...\n",
      "Processing CID 26331...\n",
      "Processing CID 27458...\n",
      "Processing CID 31209...\n",
      "Processing CID 31234...\n",
      "Processing CID 31244...\n",
      "Processing CID 31246...\n",
      "Processing CID 31249...\n",
      "Processing CID 31260...\n",
      "Processing CID 31265...\n",
      "Processing CID 31272...\n",
      "Processing CID 32594...\n",
      "Processing CID 36822...\n",
      "Processing CID 61048...\n",
      "Processing CID 61052...\n",
      "Processing CID 61138...\n",
      "Processing CID 61199...\n",
      "Processing CID 61204...\n",
      "Processing CID 61527...\n",
      "Processing CID 61653...\n",
      "Processing CID 61670...\n",
      "Processing CID 61918...\n",
      "Processing CID 62375...\n",
      "Processing CID 62378...\n",
      "Processing CID 62444...\n",
      "Processing CID 62835...\n",
      "Processing CID 62902...\n",
      "Processing CID 78925...\n",
      "Processing CID 82227...\n",
      "Processing CID 89440...\n",
      "Processing CID 170833...\n",
      "Processing CID 235414...\n",
      "Processing CID 246728...\n",
      "Processing CID 439570...\n",
      "Processing CID 440967...\n",
      "Processing CID 444539...\n",
      "Processing CID 520191...\n",
      "Processing CID 556940...\n",
      "Processing CID 637563...\n",
      "Processing CID 637566...\n",
      "Processing CID 637796...\n",
      "Processing CID 638014...\n",
      "Processing CID 643820...\n",
      "Processing CID 3578033...\n",
      "Processing CID 5315892...\n",
      "Processing CID 5365027...\n",
      "Processing CID 5366244...\n",
      "Processing CID 6999977...\n",
      "Saving to embeddingstoken/sagar2023_MoLFormer-XL-both-10pct_per_token_embeddings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting seyonec/ChemBERTa-zinc-base-v1 PER-TOKEN embeddings for sagar2023 using smiles...\n",
      "Processing CID 126...\n",
      "Processing CID 177...\n",
      "Processing CID 196...\n",
      "Processing CID 239...\n",
      "Processing CID 240...\n",
      "Processing CID 261...\n",
      "Processing CID 263...\n",
      "Processing CID 325...\n",
      "Processing CID 326...\n",
      "Processing CID 356...\n",
      "Processing CID 379...\n",
      "Processing CID 460...\n",
      "Processing CID 650...\n",
      "Processing CID 660...\n",
      "Processing CID 957...\n",
      "Processing CID 1001...\n",
      "Processing CID 1032...\n",
      "Processing CID 1049...\n",
      "Processing CID 1068...\n",
      "Processing CID 1110...\n",
      "Processing CID 1136...\n",
      "Processing CID 2214...\n",
      "Processing CID 2345...\n",
      "Processing CID 2346...\n",
      "Processing CID 2758...\n",
      "Processing CID 2879...\n",
      "Processing CID 2969...\n",
      "Processing CID 3893...\n",
      "Processing CID 4133...\n",
      "Processing CID 5541...\n",
      "Processing CID 5610...\n",
      "Processing CID 5779...\n",
      "Processing CID 5780...\n",
      "Processing CID 5960...\n",
      "Processing CID 6050...\n",
      "Processing CID 6054...\n",
      "Processing CID 6106...\n",
      "Processing CID 6184...\n",
      "Processing CID 6448...\n",
      "Processing CID 6501...\n",
      "Processing CID 6549...\n",
      "Processing CID 6584...\n",
      "Processing CID 6590...\n",
      "Processing CID 6658...\n",
      "Processing CID 6943...\n",
      "Processing CID 6997...\n",
      "Processing CID 6998...\n",
      "Processing CID 7059...\n",
      "Processing CID 7095...\n",
      "Processing CID 7122...\n",
      "Processing CID 7136...\n",
      "Processing CID 7144...\n",
      "Processing CID 7151...\n",
      "Processing CID 7165...\n",
      "Processing CID 7335...\n",
      "Processing CID 7361...\n",
      "Processing CID 7410...\n",
      "Processing CID 7463...\n",
      "Processing CID 7500...\n",
      "Processing CID 7519...\n",
      "Processing CID 7593...\n",
      "Processing CID 7635...\n",
      "Processing CID 7654...\n",
      "Processing CID 7720...\n",
      "Processing CID 7731...\n",
      "Processing CID 7749...\n",
      "Processing CID 7761...\n",
      "Processing CID 7762...\n",
      "Processing CID 7795...\n",
      "Processing CID 7799...\n",
      "Processing CID 7803...\n",
      "Processing CID 7824...\n",
      "Processing CID 7937...\n",
      "Processing CID 7967...\n",
      "Processing CID 7983...\n",
      "Processing CID 7991...\n",
      "Processing CID 8077...\n",
      "Processing CID 8082...\n",
      "Processing CID 8093...\n",
      "Processing CID 8103...\n",
      "Processing CID 8118...\n",
      "Processing CID 8129...\n",
      "Processing CID 8174...\n",
      "Processing CID 8184...\n",
      "Processing CID 8186...\n",
      "Processing CID 8193...\n",
      "Processing CID 8375...\n",
      "Processing CID 8456...\n",
      "Processing CID 8658...\n",
      "Processing CID 8697...\n",
      "Processing CID 8712...\n",
      "Processing CID 8723...\n",
      "Processing CID 8785...\n",
      "Processing CID 8797...\n",
      "Processing CID 8857...\n",
      "Processing CID 8892...\n",
      "Processing CID 8908...\n",
      "Processing CID 8914...\n",
      "Processing CID 8918...\n",
      "Processing CID 9025...\n",
      "Processing CID 9589...\n",
      "Processing CID 9609...\n",
      "Processing CID 9862...\n",
      "Processing CID 10364...\n",
      "Processing CID 10400...\n",
      "Processing CID 10430...\n",
      "Processing CID 10448...\n",
      "Processing CID 10882...\n",
      "Processing CID 10890...\n",
      "Processing CID 10895...\n",
      "Processing CID 10925...\n",
      "Processing CID 11086...\n",
      "Processing CID 11428...\n",
      "Processing CID 11525...\n",
      "Processing CID 11527...\n",
      "Processing CID 11529...\n",
      "Processing CID 11569...\n",
      "Processing CID 11583...\n",
      "Processing CID 11614...\n",
      "Processing CID 11902...\n",
      "Processing CID 11980...\n",
      "Processing CID 12180...\n",
      "Processing CID 12297...\n",
      "Processing CID 12327...\n",
      "Processing CID 12587...\n",
      "Processing CID 12741...\n",
      "Processing CID 12810...\n",
      "Processing CID 12813...\n",
      "Processing CID 14257...\n",
      "Processing CID 14286...\n",
      "Processing CID 14296...\n",
      "Processing CID 14491...\n",
      "Processing CID 14514...\n",
      "Processing CID 14525...\n",
      "Processing CID 15037...\n",
      "Processing CID 15380...\n",
      "Processing CID 15510...\n",
      "Processing CID 16741...\n",
      "Processing CID 17525...\n",
      "Processing CID 17617...\n",
      "Processing CID 18635...\n",
      "Processing CID 18827...\n",
      "Processing CID 21057...\n",
      "Processing CID 21648...\n",
      "Processing CID 22873...\n",
      "Processing CID 23235...\n",
      "Processing CID 23642...\n",
      "Processing CID 24834...\n",
      "Processing CID 26331...\n",
      "Processing CID 27458...\n",
      "Processing CID 31209...\n",
      "Processing CID 31234...\n",
      "Processing CID 31244...\n",
      "Processing CID 31246...\n",
      "Processing CID 31249...\n",
      "Processing CID 31260...\n",
      "Processing CID 31265...\n",
      "Processing CID 31272...\n",
      "Processing CID 32594...\n",
      "Processing CID 36822...\n",
      "Processing CID 61048...\n",
      "Processing CID 61052...\n",
      "Processing CID 61138...\n",
      "Processing CID 61199...\n",
      "Processing CID 61204...\n",
      "Processing CID 61527...\n",
      "Processing CID 61653...\n",
      "Processing CID 61670...\n",
      "Processing CID 61918...\n",
      "Processing CID 62375...\n",
      "Processing CID 62378...\n",
      "Processing CID 62444...\n",
      "Processing CID 62835...\n",
      "Processing CID 62902...\n",
      "Processing CID 78925...\n",
      "Processing CID 82227...\n",
      "Processing CID 89440...\n",
      "Processing CID 170833...\n",
      "Processing CID 235414...\n",
      "Processing CID 246728...\n",
      "Processing CID 439570...\n",
      "Processing CID 440967...\n",
      "Processing CID 444539...\n",
      "Processing CID 520191...\n",
      "Processing CID 556940...\n",
      "Processing CID 637563...\n",
      "Processing CID 637566...\n",
      "Processing CID 637796...\n",
      "Processing CID 638014...\n",
      "Processing CID 643820...\n",
      "Processing CID 3578033...\n",
      "Processing CID 5315892...\n",
      "Processing CID 5365027...\n",
      "Processing CID 5366244...\n",
      "Processing CID 6999977...\n",
      "Saving to embeddingstoken/sagar2023_ChemBERTa-zinc-base-v1_per_token_embeddings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jonghyunlee/ChemBERT_ChEMBL_pretrained and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting jonghyunlee/ChemBERT_ChEMBL_pretrained PER-TOKEN embeddings for sagar2023 using smiles...\n",
      "Processing CID 126...\n",
      "Processing CID 177...\n",
      "Processing CID 196...\n",
      "Processing CID 239...\n",
      "Processing CID 240...\n",
      "Processing CID 261...\n",
      "Processing CID 263...\n",
      "Processing CID 325...\n",
      "Processing CID 326...\n",
      "Processing CID 356...\n",
      "Processing CID 379...\n",
      "Processing CID 460...\n",
      "Processing CID 650...\n",
      "Processing CID 660...\n",
      "Processing CID 957...\n",
      "Processing CID 1001...\n",
      "Processing CID 1032...\n",
      "Processing CID 1049...\n",
      "Processing CID 1068...\n",
      "Processing CID 1110...\n",
      "Processing CID 1136...\n",
      "Processing CID 2214...\n",
      "Processing CID 2345...\n",
      "Processing CID 2346...\n",
      "Processing CID 2758...\n",
      "Processing CID 2879...\n",
      "Processing CID 2969...\n",
      "Processing CID 3893...\n",
      "Processing CID 4133...\n",
      "Processing CID 5541...\n",
      "Processing CID 5610...\n",
      "Processing CID 5779...\n",
      "Processing CID 5780...\n",
      "Processing CID 5960...\n",
      "Processing CID 6050...\n",
      "Processing CID 6054...\n",
      "Processing CID 6106...\n",
      "Processing CID 6184...\n",
      "Processing CID 6448...\n",
      "Processing CID 6501...\n",
      "Processing CID 6549...\n",
      "Processing CID 6584...\n",
      "Processing CID 6590...\n",
      "Processing CID 6658...\n",
      "Processing CID 6943...\n",
      "Processing CID 6997...\n",
      "Processing CID 6998...\n",
      "Processing CID 7059...\n",
      "Processing CID 7095...\n",
      "Processing CID 7122...\n",
      "Processing CID 7136...\n",
      "Processing CID 7144...\n",
      "Processing CID 7151...\n",
      "Processing CID 7165...\n",
      "Processing CID 7335...\n",
      "Processing CID 7361...\n",
      "Processing CID 7410...\n",
      "Processing CID 7463...\n",
      "Processing CID 7500...\n",
      "Processing CID 7519...\n",
      "Processing CID 7593...\n",
      "Processing CID 7635...\n",
      "Processing CID 7654...\n",
      "Processing CID 7720...\n",
      "Processing CID 7731...\n",
      "Processing CID 7749...\n",
      "Processing CID 7761...\n",
      "Processing CID 7762...\n",
      "Processing CID 7795...\n",
      "Processing CID 7799...\n",
      "Processing CID 7803...\n",
      "Processing CID 7824...\n",
      "Processing CID 7937...\n",
      "Processing CID 7967...\n",
      "Processing CID 7983...\n",
      "Processing CID 7991...\n",
      "Processing CID 8077...\n",
      "Processing CID 8082...\n",
      "Processing CID 8093...\n",
      "Processing CID 8103...\n",
      "Processing CID 8118...\n",
      "Processing CID 8129...\n",
      "Processing CID 8174...\n",
      "Processing CID 8184...\n",
      "Processing CID 8186...\n",
      "Processing CID 8193...\n",
      "Processing CID 8375...\n",
      "Processing CID 8456...\n",
      "Processing CID 8658...\n",
      "Processing CID 8697...\n",
      "Processing CID 8712...\n",
      "Processing CID 8723...\n",
      "Processing CID 8785...\n",
      "Processing CID 8797...\n",
      "Processing CID 8857...\n",
      "Processing CID 8892...\n",
      "Processing CID 8908...\n",
      "Processing CID 8914...\n",
      "Processing CID 8918...\n",
      "Processing CID 9025...\n",
      "Processing CID 9589...\n",
      "Processing CID 9609...\n",
      "Processing CID 9862...\n",
      "Processing CID 10364...\n",
      "Processing CID 10400...\n",
      "Processing CID 10430...\n",
      "Processing CID 10448...\n",
      "Processing CID 10882...\n",
      "Processing CID 10890...\n",
      "Processing CID 10895...\n",
      "Processing CID 10925...\n",
      "Processing CID 11086...\n",
      "Processing CID 11428...\n",
      "Processing CID 11525...\n",
      "Processing CID 11527...\n",
      "Processing CID 11529...\n",
      "Processing CID 11569...\n",
      "Processing CID 11583...\n",
      "Processing CID 11614...\n",
      "Processing CID 11902...\n",
      "Processing CID 11980...\n",
      "Processing CID 12180...\n",
      "Processing CID 12297...\n",
      "Processing CID 12327...\n",
      "Processing CID 12587...\n",
      "Processing CID 12741...\n",
      "Processing CID 12810...\n",
      "Processing CID 12813...\n",
      "Processing CID 14257...\n",
      "Processing CID 14286...\n",
      "Processing CID 14296...\n",
      "Processing CID 14491...\n",
      "Processing CID 14514...\n",
      "Processing CID 14525...\n",
      "Processing CID 15037...\n",
      "Processing CID 15380...\n",
      "Processing CID 15510...\n",
      "Processing CID 16741...\n",
      "Processing CID 17525...\n",
      "Processing CID 17617...\n",
      "Processing CID 18635...\n",
      "Processing CID 18827...\n",
      "Processing CID 21057...\n",
      "Processing CID 21648...\n",
      "Processing CID 22873...\n",
      "Processing CID 23235...\n",
      "Processing CID 23642...\n",
      "Processing CID 24834...\n",
      "Processing CID 26331...\n",
      "Processing CID 27458...\n",
      "Processing CID 31209...\n",
      "Processing CID 31234...\n",
      "Processing CID 31244...\n",
      "Processing CID 31246...\n",
      "Processing CID 31249...\n",
      "Processing CID 31260...\n",
      "Processing CID 31265...\n",
      "Processing CID 31272...\n",
      "Processing CID 32594...\n",
      "Processing CID 36822...\n",
      "Processing CID 61048...\n",
      "Processing CID 61052...\n",
      "Processing CID 61138...\n",
      "Processing CID 61199...\n",
      "Processing CID 61204...\n",
      "Processing CID 61527...\n",
      "Processing CID 61653...\n",
      "Processing CID 61670...\n",
      "Processing CID 61918...\n",
      "Processing CID 62375...\n",
      "Processing CID 62378...\n",
      "Processing CID 62444...\n",
      "Processing CID 62835...\n",
      "Processing CID 62902...\n",
      "Processing CID 78925...\n",
      "Processing CID 82227...\n",
      "Processing CID 89440...\n",
      "Processing CID 170833...\n",
      "Processing CID 235414...\n",
      "Processing CID 246728...\n",
      "Processing CID 439570...\n",
      "Processing CID 440967...\n",
      "Processing CID 444539...\n",
      "Processing CID 520191...\n",
      "Processing CID 556940...\n",
      "Processing CID 637563...\n",
      "Processing CID 637566...\n",
      "Processing CID 637796...\n",
      "Processing CID 638014...\n",
      "Processing CID 643820...\n",
      "Processing CID 3578033...\n",
      "Processing CID 5315892...\n",
      "Processing CID 5365027...\n",
      "Processing CID 5366244...\n",
      "Processing CID 6999977...\n",
      "Saving to embeddingstoken/sagar2023_ChemBERT_ChEMBL_pretrained_per_token_embeddings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at HUBioDataLab/SELFormer and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting HUBioDataLab/SELFormer PER-TOKEN embeddings for sagar2023 using selfies...\n",
      "Processing CID 126...\n",
      "Processing CID 177...\n",
      "Processing CID 196...\n",
      "Processing CID 239...\n",
      "Processing CID 240...\n",
      "Processing CID 261...\n",
      "Processing CID 263...\n",
      "Processing CID 325...\n",
      "Processing CID 326...\n",
      "Processing CID 356...\n",
      "Processing CID 379...\n",
      "Processing CID 460...\n",
      "Processing CID 650...\n",
      "Processing CID 660...\n",
      "Processing CID 957...\n",
      "Processing CID 1001...\n",
      "Processing CID 1032...\n",
      "Processing CID 1049...\n",
      "Processing CID 1068...\n",
      "Processing CID 1110...\n",
      "Processing CID 1136...\n",
      "Processing CID 2214...\n",
      "Processing CID 2345...\n",
      "Processing CID 2346...\n",
      "Processing CID 2758...\n",
      "Processing CID 2879...\n",
      "Processing CID 2969...\n",
      "Processing CID 3893...\n",
      "Processing CID 4133...\n",
      "Processing CID 5541...\n",
      "Processing CID 5610...\n",
      "Processing CID 5779...\n",
      "Processing CID 5780...\n",
      "Processing CID 5960...\n",
      "Processing CID 6050...\n",
      "Processing CID 6054...\n",
      "Processing CID 6106...\n",
      "Processing CID 6184...\n",
      "Processing CID 6448...\n",
      "Processing CID 6501...\n",
      "Processing CID 6549...\n",
      "Processing CID 6584...\n",
      "Processing CID 6590...\n",
      "Processing CID 6658...\n",
      "Processing CID 6943...\n",
      "Processing CID 6997...\n",
      "Processing CID 6998...\n",
      "Processing CID 7059...\n",
      "Processing CID 7095...\n",
      "Processing CID 7122...\n",
      "Processing CID 7136...\n",
      "Processing CID 7144...\n",
      "Processing CID 7151...\n",
      "Processing CID 7165...\n",
      "Processing CID 7335...\n",
      "Processing CID 7361...\n",
      "Processing CID 7410...\n",
      "Processing CID 7463...\n",
      "Processing CID 7500...\n",
      "Processing CID 7519...\n",
      "Processing CID 7593...\n",
      "Processing CID 7635...\n",
      "Processing CID 7654...\n",
      "Processing CID 7720...\n",
      "Processing CID 7731...\n",
      "Processing CID 7749...\n",
      "Processing CID 7761...\n",
      "Processing CID 7762...\n",
      "Processing CID 7795...\n",
      "Processing CID 7799...\n",
      "Processing CID 7803...\n",
      "Processing CID 7824...\n",
      "Processing CID 7937...\n",
      "Processing CID 7967...\n",
      "Processing CID 7983...\n",
      "Processing CID 7991...\n",
      "Processing CID 8077...\n",
      "Processing CID 8082...\n",
      "Processing CID 8093...\n",
      "Processing CID 8103...\n",
      "Processing CID 8118...\n",
      "Processing CID 8129...\n",
      "Processing CID 8174...\n",
      "Processing CID 8184...\n",
      "Processing CID 8186...\n",
      "Processing CID 8193...\n",
      "Processing CID 8375...\n",
      "Processing CID 8456...\n",
      "Processing CID 8658...\n",
      "Processing CID 8697...\n",
      "Processing CID 8712...\n",
      "Processing CID 8723...\n",
      "Processing CID 8785...\n",
      "Processing CID 8797...\n",
      "Processing CID 8857...\n",
      "Processing CID 8892...\n",
      "Processing CID 8908...\n",
      "Processing CID 8914...\n",
      "Processing CID 8918...\n",
      "Processing CID 9025...\n",
      "Processing CID 9589...\n",
      "Processing CID 9609...\n",
      "Processing CID 9862...\n",
      "Processing CID 10364...\n",
      "Processing CID 10400...\n",
      "Processing CID 10430...\n",
      "Processing CID 10448...\n",
      "Processing CID 10882...\n",
      "Processing CID 10890...\n",
      "Processing CID 10895...\n",
      "Processing CID 10925...\n",
      "Processing CID 11086...\n",
      "Processing CID 11428...\n",
      "Processing CID 11525...\n",
      "Processing CID 11527...\n",
      "Processing CID 11529...\n",
      "Processing CID 11569...\n",
      "Processing CID 11583...\n",
      "Processing CID 11614...\n",
      "Processing CID 11902...\n",
      "Processing CID 11980...\n",
      "Processing CID 12180...\n",
      "Processing CID 12297...\n",
      "Processing CID 12327...\n",
      "Processing CID 12587...\n",
      "Processing CID 12741...\n",
      "Processing CID 12810...\n",
      "Processing CID 12813...\n",
      "Processing CID 14257...\n",
      "Processing CID 14286...\n",
      "Processing CID 14296...\n",
      "Processing CID 14491...\n",
      "Processing CID 14514...\n",
      "Processing CID 14525...\n",
      "Processing CID 15037...\n",
      "Processing CID 15380...\n",
      "Processing CID 15510...\n",
      "Processing CID 16741...\n",
      "Processing CID 17525...\n",
      "Processing CID 17617...\n",
      "Processing CID 18635...\n",
      "Processing CID 18827...\n",
      "Processing CID 21057...\n",
      "Processing CID 21648...\n",
      "Processing CID 22873...\n",
      "Processing CID 23235...\n",
      "Processing CID 23642...\n",
      "Processing CID 24834...\n",
      "Processing CID 26331...\n",
      "Processing CID 27458...\n",
      "Processing CID 31209...\n",
      "Processing CID 31234...\n",
      "Processing CID 31244...\n",
      "Processing CID 31246...\n",
      "Processing CID 31249...\n",
      "Processing CID 31260...\n",
      "Processing CID 31265...\n",
      "Processing CID 31272...\n",
      "Processing CID 32594...\n",
      "Processing CID 36822...\n",
      "Processing CID 61048...\n",
      "Processing CID 61052...\n",
      "Processing CID 61138...\n",
      "Processing CID 61199...\n",
      "Processing CID 61204...\n",
      "Processing CID 61527...\n",
      "Processing CID 61653...\n",
      "Processing CID 61670...\n",
      "Processing CID 61918...\n",
      "Processing CID 62375...\n",
      "Processing CID 62378...\n",
      "Processing CID 62444...\n",
      "Processing CID 62835...\n",
      "Processing CID 62902...\n",
      "Processing CID 78925...\n",
      "Processing CID 82227...\n",
      "Processing CID 89440...\n",
      "Processing CID 170833...\n",
      "Processing CID 235414...\n",
      "Processing CID 246728...\n",
      "Processing CID 439570...\n",
      "Processing CID 440967...\n",
      "Processing CID 444539...\n",
      "Processing CID 520191...\n",
      "Processing CID 556940...\n",
      "Processing CID 637563...\n",
      "Processing CID 637566...\n",
      "Processing CID 637796...\n",
      "Processing CID 638014...\n",
      "Processing CID 643820...\n",
      "Processing CID 3578033...\n",
      "Processing CID 5315892...\n",
      "Processing CID 5365027...\n",
      "Processing CID 5366244...\n",
      "Processing CID 6999977...\n",
      "Saving to embeddingstoken/sagar2023_SELFormer_per_token_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "Input_types = {\n",
    "    'ibm/MoLFormer-XL-both-10pct':'smiles',\n",
    "    'seyonec/ChemBERTa-zinc-base-v1':'smiles',\n",
    "    'jonghyunlee/ChemBERT_ChEMBL_pretrained':'smiles',\n",
    "    \"HUBioDataLab/SELFormer\":'selfies'\n",
    "    }\n",
    "\n",
    "for model_name in ['ibm/MoLFormer-XL-both-10pct',\n",
    "                   'seyonec/ChemBERTa-zinc-base-v1',\"jonghyunlee/ChemBERT_ChEMBL_pretrained\",\"HUBioDataLab/SELFormer\"\n",
    "                   ]:\n",
    "    for ds in [\n",
    "        'sagar2023'\n",
    "        # ,'keller2016',\n",
    "        # 'bierling2025',\n",
    "            #    'leffingwell'\n",
    "               ]:\n",
    "    \n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True,use_safetensors=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        # extract_representations(tokenizer, model,model_name,save_path='avgtokenembeddings',ds=ds,input_type=Input_types[model_name],mean=True)\n",
    "        extract_representations_per_token(tokenizer, model,model_name,save_path='embeddingstoken',ds=ds,input_type=Input_types[model_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6889577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting morgan fingerprints for dataset 'sagar2023' (SMILES only)...\n",
      "Processing CID 126...\n",
      "Processing CID 177...\n",
      "Processing CID 196...\n",
      "Processing CID 239...\n",
      "Processing CID 240...\n",
      "Processing CID 261...\n",
      "Processing CID 263...\n",
      "Processing CID 325...\n",
      "Processing CID 326...\n",
      "Processing CID 356...\n",
      "Processing CID 379...\n",
      "Processing CID 460...\n",
      "Processing CID 650...\n",
      "Processing CID 660...\n",
      "Processing CID 957...\n",
      "Processing CID 1001...\n",
      "Processing CID 1032...\n",
      "Processing CID 1049...\n",
      "Processing CID 1068...\n",
      "Processing CID 1110...\n",
      "Processing CID 1136...\n",
      "Processing CID 2214...\n",
      "Processing CID 2345...\n",
      "Processing CID 2346...\n",
      "Processing CID 2758...\n",
      "Processing CID 2879...\n",
      "Processing CID 2969...\n",
      "Processing CID 3893...\n",
      "Processing CID 4133...\n",
      "Processing CID 5541...\n",
      "Processing CID 5610...\n",
      "Processing CID 5779...\n",
      "Processing CID 5780...\n",
      "Processing CID 5960...\n",
      "Processing CID 6050...\n",
      "Processing CID 6054...\n",
      "Processing CID 6106...\n",
      "Processing CID 6184...\n",
      "Processing CID 6448...\n",
      "Processing CID 6501...\n",
      "Processing CID 6549...\n",
      "Processing CID 6584...\n",
      "Processing CID 6590...\n",
      "Processing CID 6658...\n",
      "Processing CID 6943...\n",
      "Processing CID 6997...\n",
      "Processing CID 6998...\n",
      "Processing CID 7059...\n",
      "Processing CID 7095...\n",
      "Processing CID 7122...\n",
      "Processing CID 7136...\n",
      "Processing CID 7144...\n",
      "Processing CID 7151...\n",
      "Processing CID 7165...\n",
      "Processing CID 7335...\n",
      "Processing CID 7361...\n",
      "Processing CID 7410...\n",
      "Processing CID 7463...\n",
      "Processing CID 7500...\n",
      "Processing CID 7519...\n",
      "Processing CID 7593...\n",
      "Processing CID 7635...\n",
      "Processing CID 7654...\n",
      "Processing CID 7720...\n",
      "Processing CID 7731...\n",
      "Processing CID 7749...\n",
      "Processing CID 7761...\n",
      "Processing CID 7762...\n",
      "Processing CID 7795...\n",
      "Processing CID 7799...\n",
      "Processing CID 7803...\n",
      "Processing CID 7824...\n",
      "Processing CID 7937...\n",
      "Processing CID 7967...\n",
      "Processing CID 7983...\n",
      "Processing CID 7991...\n",
      "Processing CID 8077...\n",
      "Processing CID 8082...\n",
      "Processing CID 8093...\n",
      "Processing CID 8103...\n",
      "Processing CID 8118...\n",
      "Processing CID 8129...\n",
      "Processing CID 8174...\n",
      "Processing CID 8184...\n",
      "Processing CID 8186...\n",
      "Processing CID 8193...\n",
      "Processing CID 8375...\n",
      "Processing CID 8456...\n",
      "Processing CID 8658...\n",
      "Processing CID 8697...\n",
      "Processing CID 8712...\n",
      "Processing CID 8723...\n",
      "Processing CID 8785...\n",
      "Processing CID 8797...\n",
      "Processing CID 8857...\n",
      "Processing CID 8892...\n",
      "Processing CID 8908...\n",
      "Processing CID 8914...\n",
      "Processing CID 8918...\n",
      "Processing CID 9025...\n",
      "Processing CID 9589...\n",
      "Processing CID 9609...\n",
      "Processing CID 9862...\n",
      "Processing CID 10364...\n",
      "Processing CID 10400...\n",
      "Processing CID 10430...\n",
      "Processing CID 10448...\n",
      "Processing CID 10882...\n",
      "Processing CID 10890...\n",
      "Processing CID 10895...\n",
      "Processing CID 10925...\n",
      "Processing CID 11086...\n",
      "Processing CID 11428...\n",
      "Processing CID 11525...\n",
      "Processing CID 11527...\n",
      "Processing CID 11529...\n",
      "Processing CID 11569...\n",
      "Processing CID 11583...\n",
      "Processing CID 11614...\n",
      "Processing CID 11902...\n",
      "Processing CID 11980...\n",
      "Processing CID 12180...\n",
      "Processing CID 12297...\n",
      "Processing CID 12327...\n",
      "Processing CID 12587...\n",
      "Processing CID 12741...\n",
      "Processing CID 12810...\n",
      "Processing CID 12813...\n",
      "Processing CID 14257...\n",
      "Processing CID 14286...\n",
      "Processing CID 14296...\n",
      "Processing CID 14491...\n",
      "Processing CID 14514...\n",
      "Processing CID 14525...\n",
      "Processing CID 15037...\n",
      "Processing CID 15380...\n",
      "Processing CID 15510...\n",
      "Processing CID 16741...\n",
      "Processing CID 17525...\n",
      "Processing CID 17617...\n",
      "Processing CID 18635...\n",
      "Processing CID 18827...\n",
      "Processing CID 21057...\n",
      "Processing CID 21648...\n",
      "Processing CID 22873...\n",
      "Processing CID 23235...\n",
      "Processing CID 23642...\n",
      "Processing CID 24834...\n",
      "Processing CID 26331...\n",
      "Processing CID 27458...\n",
      "Processing CID 31209...\n",
      "Processing CID 31234...\n",
      "Processing CID 31244...\n",
      "Processing CID 31246...\n",
      "Processing CID 31249...\n",
      "Processing CID 31260...\n",
      "Processing CID 31265...\n",
      "Processing CID 31272...\n",
      "Processing CID 32594...\n",
      "Processing CID 36822...\n",
      "Processing CID 61048...\n",
      "Processing CID 61052...\n",
      "Processing CID 61138...\n",
      "Processing CID 61199...\n",
      "Processing CID 61204...\n",
      "Processing CID 61527...\n",
      "Processing CID 61653...\n",
      "Processing CID 61670...\n",
      "Processing CID 61918...\n",
      "Processing CID 62375...\n",
      "Processing CID 62378...\n",
      "Processing CID 62444...\n",
      "Processing CID 62835...\n",
      "Processing CID 62902...\n",
      "Processing CID 78925...\n",
      "Processing CID 82227...\n",
      "Processing CID 89440...\n",
      "Processing CID 170833...\n",
      "Processing CID 235414...\n",
      "Processing CID 246728...\n",
      "Processing CID 439570...\n",
      "Processing CID 440967...\n",
      "Processing CID 444539...\n",
      "Processing CID 520191...\n",
      "Processing CID 556940...\n",
      "Processing CID 637563...\n",
      "Processing CID 637566...\n",
      "Processing CID 637796...\n",
      "Processing CID 638014...\n",
      "Processing CID 643820...\n",
      "Processing CID 3578033...\n",
      "Processing CID 5315892...\n",
      "Processing CID 5365027...\n",
      "Processing CID 5366244...\n",
      "Processing CID 6999977...\n",
      "Saving to fingerprints/sagar2023_Morgan_r2_1024_embeddings.csv\n",
      "Extracting rdkit fingerprints for dataset 'sagar2023' (SMILES only)...\n",
      "Processing CID 126...\n",
      "Processing CID 177...\n",
      "Processing CID 196...\n",
      "Processing CID 239...\n",
      "Processing CID 240...\n",
      "Processing CID 261...\n",
      "Processing CID 263...\n",
      "Processing CID 325...\n",
      "Processing CID 326...\n",
      "Processing CID 356...\n",
      "Processing CID 379...\n",
      "Processing CID 460...\n",
      "Processing CID 650...\n",
      "Processing CID 660...\n",
      "Processing CID 957...\n",
      "Processing CID 1001...\n",
      "Processing CID 1032...\n",
      "Processing CID 1049...\n",
      "Processing CID 1068...\n",
      "Processing CID 1110...\n",
      "Processing CID 1136...\n",
      "Processing CID 2214...\n",
      "Processing CID 2345...\n",
      "Processing CID 2346...\n",
      "Processing CID 2758...\n",
      "Processing CID 2879...\n",
      "Processing CID 2969...\n",
      "Processing CID 3893...\n",
      "Processing CID 4133...\n",
      "Processing CID 5541...\n",
      "Processing CID 5610...\n",
      "Processing CID 5779...\n",
      "Processing CID 5780...\n",
      "Processing CID 5960...\n",
      "Processing CID 6050...\n",
      "Processing CID 6054...\n",
      "Processing CID 6106...\n",
      "Processing CID 6184...\n",
      "Processing CID 6448...\n",
      "Processing CID 6501...\n",
      "Processing CID 6549...\n",
      "Processing CID 6584...\n",
      "Processing CID 6590...\n",
      "Processing CID 6658...\n",
      "Processing CID 6943...\n",
      "Processing CID 6997...\n",
      "Processing CID 6998...\n",
      "Processing CID 7059...\n",
      "Processing CID 7095...\n",
      "Processing CID 7122...\n",
      "Processing CID 7136...\n",
      "Processing CID 7144...\n",
      "Processing CID 7151...\n",
      "Processing CID 7165...\n",
      "Processing CID 7335...\n",
      "Processing CID 7361...\n",
      "Processing CID 7410...\n",
      "Processing CID 7463...\n",
      "Processing CID 7500...\n",
      "Processing CID 7519...\n",
      "Processing CID 7593...\n",
      "Processing CID 7635...\n",
      "Processing CID 7654...\n",
      "Processing CID 7720...\n",
      "Processing CID 7731...\n",
      "Processing CID 7749...\n",
      "Processing CID 7761...\n",
      "Processing CID 7762...\n",
      "Processing CID 7795...\n",
      "Processing CID 7799...\n",
      "Processing CID 7803...\n",
      "Processing CID 7824...\n",
      "Processing CID 7937...\n",
      "Processing CID 7967...\n",
      "Processing CID 7983...\n",
      "Processing CID 7991...\n",
      "Processing CID 8077...\n",
      "Processing CID 8082...\n",
      "Processing CID 8093...\n",
      "Processing CID 8103...\n",
      "Processing CID 8118...\n",
      "Processing CID 8129...\n",
      "Processing CID 8174...\n",
      "Processing CID 8184...\n",
      "Processing CID 8186...\n",
      "Processing CID 8193...\n",
      "Processing CID 8375...\n",
      "Processing CID 8456...\n",
      "Processing CID 8658...\n",
      "Processing CID 8697...\n",
      "Processing CID 8712...\n",
      "Processing CID 8723...\n",
      "Processing CID 8785...\n",
      "Processing CID 8797...\n",
      "Processing CID 8857...\n",
      "Processing CID 8892...\n",
      "Processing CID 8908...\n",
      "Processing CID 8914...\n",
      "Processing CID 8918...\n",
      "Processing CID 9025...\n",
      "Processing CID 9589...\n",
      "Processing CID 9609...\n",
      "Processing CID 9862...\n",
      "Processing CID 10364...\n",
      "Processing CID 10400...\n",
      "Processing CID 10430...\n",
      "Processing CID 10448...\n",
      "Processing CID 10882...\n",
      "Processing CID 10890...\n",
      "Processing CID 10895...\n",
      "Processing CID 10925...\n",
      "Processing CID 11086...\n",
      "Processing CID 11428...\n",
      "Processing CID 11525...\n",
      "Processing CID 11527...\n",
      "Processing CID 11529...\n",
      "Processing CID 11569...\n",
      "Processing CID 11583...\n",
      "Processing CID 11614...\n",
      "Processing CID 11902...\n",
      "Processing CID 11980...\n",
      "Processing CID 12180...\n",
      "Processing CID 12297...\n",
      "Processing CID 12327...\n",
      "Processing CID 12587...\n",
      "Processing CID 12741...\n",
      "Processing CID 12810...\n",
      "Processing CID 12813...\n",
      "Processing CID 14257...\n",
      "Processing CID 14286...\n",
      "Processing CID 14296...\n",
      "Processing CID 14491...\n",
      "Processing CID 14514...\n",
      "Processing CID 14525...\n",
      "Processing CID 15037...\n",
      "Processing CID 15380...\n",
      "Processing CID 15510...\n",
      "Processing CID 16741...\n",
      "Processing CID 17525...\n",
      "Processing CID 17617...\n",
      "Processing CID 18635...\n",
      "Processing CID 18827...\n",
      "Processing CID 21057...\n",
      "Processing CID 21648...\n",
      "Processing CID 22873...\n",
      "Processing CID 23235...\n",
      "Processing CID 23642...\n",
      "Processing CID 24834...\n",
      "Processing CID 26331...\n",
      "Processing CID 27458...\n",
      "Processing CID 31209...\n",
      "Processing CID 31234...\n",
      "Processing CID 31244...\n",
      "Processing CID 31246...\n",
      "Processing CID 31249...\n",
      "Processing CID 31260...\n",
      "Processing CID 31265...\n",
      "Processing CID 31272...\n",
      "Processing CID 32594...\n",
      "Processing CID 36822...\n",
      "Processing CID 61048...\n",
      "Processing CID 61052...\n",
      "Processing CID 61138...\n",
      "Processing CID 61199...\n",
      "Processing CID 61204...\n",
      "Processing CID 61527...\n",
      "Processing CID 61653...\n",
      "Processing CID 61670...\n",
      "Processing CID 61918...\n",
      "Processing CID 62375...\n",
      "Processing CID 62378...\n",
      "Processing CID 62444...\n",
      "Processing CID 62835...\n",
      "Processing CID 62902...\n",
      "Processing CID 78925...\n",
      "Processing CID 82227...\n",
      "Processing CID 89440...\n",
      "Processing CID 170833...\n",
      "Processing CID 235414...\n",
      "Processing CID 246728...\n",
      "Processing CID 439570...\n",
      "Processing CID 440967...\n",
      "Processing CID 444539...\n",
      "Processing CID 520191...\n",
      "Processing CID 556940...\n",
      "Processing CID 637563...\n",
      "Processing CID 637566...\n",
      "Processing CID 637796...\n",
      "Processing CID 638014...\n",
      "Processing CID 643820...\n",
      "Processing CID 3578033...\n",
      "Processing CID 5315892...\n",
      "Processing CID 5365027...\n",
      "Processing CID 5366244...\n",
      "Processing CID 6999977...\n",
      "Saving to fingerprints/sagar2023_RDK_1024_embeddings.csv\n",
      "Extracting maccs fingerprints for dataset 'sagar2023' (SMILES only)...\n",
      "Processing CID 126...\n",
      "Processing CID 177...\n",
      "Processing CID 196...\n",
      "Processing CID 239...\n",
      "Processing CID 240...\n",
      "Processing CID 261...\n",
      "Processing CID 263...\n",
      "Processing CID 325...\n",
      "Processing CID 326...\n",
      "Processing CID 356...\n",
      "Processing CID 379...\n",
      "Processing CID 460...\n",
      "Processing CID 650...\n",
      "Processing CID 660...\n",
      "Processing CID 957...\n",
      "Processing CID 1001...\n",
      "Processing CID 1032...\n",
      "Processing CID 1049...\n",
      "Processing CID 1068...\n",
      "Processing CID 1110...\n",
      "Processing CID 1136...\n",
      "Processing CID 2214...\n",
      "Processing CID 2345...\n",
      "Processing CID 2346...\n",
      "Processing CID 2758...\n",
      "Processing CID 2879...\n",
      "Processing CID 2969...\n",
      "Processing CID 3893...\n",
      "Processing CID 4133...\n",
      "Processing CID 5541...\n",
      "Processing CID 5610...\n",
      "Processing CID 5779...\n",
      "Processing CID 5780...\n",
      "Processing CID 5960...\n",
      "Processing CID 6050...\n",
      "Processing CID 6054...\n",
      "Processing CID 6106...\n",
      "Processing CID 6184...\n",
      "Processing CID 6448...\n",
      "Processing CID 6501...\n",
      "Processing CID 6549...\n",
      "Processing CID 6584...\n",
      "Processing CID 6590...\n",
      "Processing CID 6658...\n",
      "Processing CID 6943...\n",
      "Processing CID 6997...\n",
      "Processing CID 6998...\n",
      "Processing CID 7059...\n",
      "Processing CID 7095...\n",
      "Processing CID 7122...\n",
      "Processing CID 7136...\n",
      "Processing CID 7144...\n",
      "Processing CID 7151...\n",
      "Processing CID 7165...\n",
      "Processing CID 7335...\n",
      "Processing CID 7361...\n",
      "Processing CID 7410...\n",
      "Processing CID 7463...\n",
      "Processing CID 7500...\n",
      "Processing CID 7519...\n",
      "Processing CID 7593...\n",
      "Processing CID 7635...\n",
      "Processing CID 7654...\n",
      "Processing CID 7720...\n",
      "Processing CID 7731...\n",
      "Processing CID 7749...\n",
      "Processing CID 7761...\n",
      "Processing CID 7762...\n",
      "Processing CID 7795...\n",
      "Processing CID 7799...\n",
      "Processing CID 7803...\n",
      "Processing CID 7824...\n",
      "Processing CID 7937...\n",
      "Processing CID 7967...\n",
      "Processing CID 7983...\n",
      "Processing CID 7991...\n",
      "Processing CID 8077...\n",
      "Processing CID 8082...\n",
      "Processing CID 8093...\n",
      "Processing CID 8103...\n",
      "Processing CID 8118...\n",
      "Processing CID 8129...\n",
      "Processing CID 8174...\n",
      "Processing CID 8184...\n",
      "Processing CID 8186...\n",
      "Processing CID 8193...\n",
      "Processing CID 8375...\n",
      "Processing CID 8456...\n",
      "Processing CID 8658...\n",
      "Processing CID 8697...\n",
      "Processing CID 8712...\n",
      "Processing CID 8723...\n",
      "Processing CID 8785...\n",
      "Processing CID 8797...\n",
      "Processing CID 8857...\n",
      "Processing CID 8892...\n",
      "Processing CID 8908...\n",
      "Processing CID 8914...\n",
      "Processing CID 8918...\n",
      "Processing CID 9025...\n",
      "Processing CID 9589...\n",
      "Processing CID 9609...\n",
      "Processing CID 9862...\n",
      "Processing CID 10364...\n",
      "Processing CID 10400...\n",
      "Processing CID 10430...\n",
      "Processing CID 10448...\n",
      "Processing CID 10882...\n",
      "Processing CID 10890...\n",
      "Processing CID 10895...\n",
      "Processing CID 10925...\n",
      "Processing CID 11086...\n",
      "Processing CID 11428...\n",
      "Processing CID 11525...\n",
      "Processing CID 11527...\n",
      "Processing CID 11529...\n",
      "Processing CID 11569...\n",
      "Processing CID 11583...\n",
      "Processing CID 11614...\n",
      "Processing CID 11902...\n",
      "Processing CID 11980...\n",
      "Processing CID 12180...\n",
      "Processing CID 12297...\n",
      "Processing CID 12327...\n",
      "Processing CID 12587...\n",
      "Processing CID 12741...\n",
      "Processing CID 12810...\n",
      "Processing CID 12813...\n",
      "Processing CID 14257...\n",
      "Processing CID 14286...\n",
      "Processing CID 14296...\n",
      "Processing CID 14491...\n",
      "Processing CID 14514...\n",
      "Processing CID 14525...\n",
      "Processing CID 15037...\n",
      "Processing CID 15380...\n",
      "Processing CID 15510...\n",
      "Processing CID 16741...\n",
      "Processing CID 17525...\n",
      "Processing CID 17617...\n",
      "Processing CID 18635...\n",
      "Processing CID 18827...\n",
      "Processing CID 21057...\n",
      "Processing CID 21648...\n",
      "Processing CID 22873...\n",
      "Processing CID 23235...\n",
      "Processing CID 23642...\n",
      "Processing CID 24834...\n",
      "Processing CID 26331...\n",
      "Processing CID 27458...\n",
      "Processing CID 31209...\n",
      "Processing CID 31234...\n",
      "Processing CID 31244...\n",
      "Processing CID 31246...\n",
      "Processing CID 31249...\n",
      "Processing CID 31260...\n",
      "Processing CID 31265...\n",
      "Processing CID 31272...\n",
      "Processing CID 32594...\n",
      "Processing CID 36822...\n",
      "Processing CID 61048...\n",
      "Processing CID 61052...\n",
      "Processing CID 61138...\n",
      "Processing CID 61199...\n",
      "Processing CID 61204...\n",
      "Processing CID 61527...\n",
      "Processing CID 61653...\n",
      "Processing CID 61670...\n",
      "Processing CID 61918...\n",
      "Processing CID 62375...\n",
      "Processing CID 62378...\n",
      "Processing CID 62444...\n",
      "Processing CID 62835...\n",
      "Processing CID 62902...\n",
      "Processing CID 78925...\n",
      "Processing CID 82227...\n",
      "Processing CID 89440...\n",
      "Processing CID 170833...\n",
      "Processing CID 235414...\n",
      "Processing CID 246728...\n",
      "Processing CID 439570...\n",
      "Processing CID 440967...\n",
      "Processing CID 444539...\n",
      "Processing CID 520191...\n",
      "Processing CID 556940...\n",
      "Processing CID 637563...\n",
      "Processing CID 637566...\n",
      "Processing CID 637796...\n",
      "Processing CID 638014...\n",
      "Processing CID 643820...\n",
      "Processing CID 3578033...\n",
      "Processing CID 5315892...\n",
      "Processing CID 5365027...\n",
      "Processing CID 5366244...\n",
      "Processing CID 6999977...\n",
      "Saving to fingerprints/sagar2023_MACCS_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "for fp_type in ['morgan','rdkit','maccs']:\n",
    "    for ds in [\n",
    "        'sagar2023'\n",
    "        # ,'keller2016',\n",
    "        # 'bierling2025',\n",
    "            #    'leffingwell'\n",
    "               ]:\n",
    "        extract_fingerprints(\n",
    "            ds=ds,\n",
    "            fp_type=fp_type,\n",
    "            radius=2,\n",
    "            n_bits=1024,\n",
    "            save_path=\"fingerprints\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "961f78b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting full RDKit descriptor set for dataset 'sagar2023' (SMILES only)...\n",
      "Processing CID 126...\n",
      "Processing CID 177...\n",
      "Processing CID 196...\n",
      "Processing CID 239...\n",
      "Processing CID 240...\n",
      "Processing CID 261...\n",
      "Processing CID 263...\n",
      "Processing CID 325...\n",
      "Processing CID 326...\n",
      "Processing CID 356...\n",
      "Processing CID 379...\n",
      "Processing CID 460...\n",
      "Processing CID 650...\n",
      "Processing CID 660...\n",
      "Processing CID 957...\n",
      "Processing CID 1001...\n",
      "Processing CID 1032...\n",
      "Processing CID 1049...\n",
      "Processing CID 1068...\n",
      "Processing CID 1110...\n",
      "Processing CID 1136...\n",
      "Processing CID 2214...\n",
      "Processing CID 2345...\n",
      "Processing CID 2346...\n",
      "Processing CID 2758...\n",
      "Processing CID 2879...\n",
      "Processing CID 2969...\n",
      "Processing CID 3893...\n",
      "Processing CID 4133...\n",
      "Processing CID 5541...\n",
      "Processing CID 5610...\n",
      "Processing CID 5779...\n",
      "Processing CID 5780...\n",
      "Processing CID 5960...\n",
      "Processing CID 6050...\n",
      "Processing CID 6054...\n",
      "Processing CID 6106...\n",
      "Processing CID 6184...\n",
      "Processing CID 6448...\n",
      "Processing CID 6501...\n",
      "Processing CID 6549...\n",
      "Processing CID 6584...\n",
      "Processing CID 6590...\n",
      "Processing CID 6658...\n",
      "Processing CID 6943...\n",
      "Processing CID 6997...\n",
      "Processing CID 6998...\n",
      "Processing CID 7059...\n",
      "Processing CID 7095...\n",
      "Processing CID 7122...\n",
      "Processing CID 7136...\n",
      "Processing CID 7144...\n",
      "Processing CID 7151...\n",
      "Processing CID 7165...\n",
      "Processing CID 7335...\n",
      "Processing CID 7361...\n",
      "Processing CID 7410...\n",
      "Processing CID 7463...\n",
      "Processing CID 7500...\n",
      "Processing CID 7519...\n",
      "Processing CID 7593...\n",
      "Processing CID 7635...\n",
      "Processing CID 7654...\n",
      "Processing CID 7720...\n",
      "Processing CID 7731...\n",
      "Processing CID 7749...\n",
      "Processing CID 7761...\n",
      "Processing CID 7762...\n",
      "Processing CID 7795...\n",
      "Processing CID 7799...\n",
      "Processing CID 7803...\n",
      "Processing CID 7824...\n",
      "Processing CID 7937...\n",
      "Processing CID 7967...\n",
      "Processing CID 7983...\n",
      "Processing CID 7991...\n",
      "Processing CID 8077...\n",
      "Processing CID 8082...\n",
      "Processing CID 8093...\n",
      "Processing CID 8103...\n",
      "Processing CID 8118...\n",
      "Processing CID 8129...\n",
      "Processing CID 8174...\n",
      "Processing CID 8184...\n",
      "Processing CID 8186...\n",
      "Processing CID 8193...\n",
      "Processing CID 8375...\n",
      "Processing CID 8456...\n",
      "Processing CID 8658...\n",
      "Processing CID 8697...\n",
      "Processing CID 8712...\n",
      "Processing CID 8723...\n",
      "Processing CID 8785...\n",
      "Processing CID 8797...\n",
      "Processing CID 8857...\n",
      "Processing CID 8892...\n",
      "Processing CID 8908...\n",
      "Processing CID 8914...\n",
      "Processing CID 8918...\n",
      "Processing CID 9025...\n",
      "Processing CID 9589...\n",
      "Processing CID 9609...\n",
      "Processing CID 9862...\n",
      "Processing CID 10364...\n",
      "Processing CID 10400...\n",
      "Processing CID 10430...\n",
      "Processing CID 10448...\n",
      "Processing CID 10882...\n",
      "Processing CID 10890...\n",
      "Processing CID 10895...\n",
      "Processing CID 10925...\n",
      "Processing CID 11086...\n",
      "Processing CID 11428...\n",
      "Processing CID 11525...\n",
      "Processing CID 11527...\n",
      "Processing CID 11529...\n",
      "Processing CID 11569...\n",
      "Processing CID 11583...\n",
      "Processing CID 11614...\n",
      "Processing CID 11902...\n",
      "Processing CID 11980...\n",
      "Processing CID 12180...\n",
      "Processing CID 12297...\n",
      "Processing CID 12327...\n",
      "Processing CID 12587...\n",
      "Processing CID 12741...\n",
      "Processing CID 12810...\n",
      "Processing CID 12813...\n",
      "Processing CID 14257...\n",
      "Processing CID 14286...\n",
      "Processing CID 14296...\n",
      "Processing CID 14491...\n",
      "Processing CID 14514...\n",
      "Processing CID 14525...\n",
      "Processing CID 15037...\n",
      "Processing CID 15380...\n",
      "Processing CID 15510...\n",
      "Processing CID 16741...\n",
      "Processing CID 17525...\n",
      "Processing CID 17617...\n",
      "Processing CID 18635...\n",
      "Processing CID 18827...\n",
      "Processing CID 21057...\n",
      "Processing CID 21648...\n",
      "Processing CID 22873...\n",
      "Processing CID 23235...\n",
      "Processing CID 23642...\n",
      "Processing CID 24834...\n",
      "Processing CID 26331...\n",
      "Processing CID 27458...\n",
      "Processing CID 31209...\n",
      "Processing CID 31234...\n",
      "Processing CID 31244...\n",
      "Processing CID 31246...\n",
      "Processing CID 31249...\n",
      "Processing CID 31260...\n",
      "Processing CID 31265...\n",
      "Processing CID 31272...\n",
      "Processing CID 32594...\n",
      "Processing CID 36822...\n",
      "Processing CID 61048...\n",
      "Processing CID 61052...\n",
      "Processing CID 61138...\n",
      "Processing CID 61199...\n",
      "Processing CID 61204...\n",
      "Processing CID 61527...\n",
      "Processing CID 61653...\n",
      "Processing CID 61670...\n",
      "Processing CID 61918...\n",
      "Processing CID 62375...\n",
      "Processing CID 62378...\n",
      "Processing CID 62444...\n",
      "Processing CID 62835...\n",
      "Processing CID 62902...\n",
      "Processing CID 78925...\n",
      "Processing CID 82227...\n",
      "Processing CID 89440...\n",
      "Processing CID 170833...\n",
      "Processing CID 235414...\n",
      "Processing CID 246728...\n",
      "Processing CID 439570...\n",
      "Processing CID 440967...\n",
      "Processing CID 444539...\n",
      "Processing CID 520191...\n",
      "Processing CID 556940...\n",
      "Processing CID 637563...\n",
      "Processing CID 637566...\n",
      "Processing CID 637796...\n",
      "Processing CID 638014...\n",
      "Processing CID 643820...\n",
      "Processing CID 3578033...\n",
      "Processing CID 5315892...\n",
      "Processing CID 5365027...\n",
      "Processing CID 5366244...\n",
      "Processing CID 6999977...\n",
      "Saving RDKit descriptor table to: rdkit_descriptors/sagar2023_RDKit_Descriptors.csv\n"
     ]
    }
   ],
   "source": [
    "for ds in [\n",
    "        'sagar2023'\n",
    "        # ,'keller2016',\n",
    "        # 'bierling2025',\n",
    "            #    'leffingwell'\n",
    "               ]:\n",
    "    extract_rdkit_descriptors(\n",
    "        ds=ds,\n",
    "        save_path=\"rdkit_descriptors\",\n",
    "        model_name=\"RDKit_Descriptors\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7716f02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_attention_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m model = AutoModel.from_pretrained(model_name, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m,use_safetensors=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mextract_attention_weights\u001b[49m(tokenizer, model,model_name,save_path=\u001b[33m'\u001b[39m\u001b[33mattention\u001b[39m\u001b[33m'\u001b[39m,ds=ds,input_type=Input_types[model_name])\n",
      "\u001b[31mNameError\u001b[39m: name 'extract_attention_weights' is not defined"
     ]
    }
   ],
   "source": [
    "Input_types = {\n",
    "    'ibm/MoLFormer-XL-both-10pct':'smiles',\n",
    "    'seyonec/ChemBERTa-zinc-base-v1':'smiles',\n",
    "    'jonghyunlee/ChemBERT_ChEMBL_pretrained':'smiles',\n",
    "    \"HUBioDataLab/SELFormer\":'selfies'\n",
    "    }\n",
    "\n",
    "for model_name in ['ibm/MoLFormer-XL-both-10pct',\n",
    "    # 'seyonec/ChemBERTa-zinc-base-v1',\"jonghyunlee/ChemBERT_ChEMBL_pretrained\",\"HUBioDataLab/SELFormer\"\n",
    "    ]:\n",
    "    for ds in [\n",
    "        'sagar2023',\n",
    "        # 'keller2016','bierling2025',\n",
    "            #    'leffingwell'\n",
    "               ]:\n",
    "    \n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True,use_safetensors=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        extract_attention_weights(tokenizer, model,model_name,save_path='attention',ds=ds,input_type=Input_types[model_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3dc719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ibm/MoLFormer-XL-both-10pct embeddings for snitz2013 (pairwise mode, smiles)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m model = AutoModel.from_pretrained(model_name, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m,use_safetensors=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     14\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mextract_representations_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mInput_types\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mextract_representations_pairs\u001b[39m\u001b[34m(tokenizer, model, model_name, ds, input_type, token, save_path)\u001b[39m\n\u001b[32m     71\u001b[39m     out = model(**inputs, output_hidden_states=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out.hidden_states  \u001b[38;5;66;03m# list of [B, T, D]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m iso_hiddens = \u001b[43mrun_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43miso_txt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m can_hiddens = run_encoder(can_txt)\n\u001b[32m     77\u001b[39m n_layers = \u001b[38;5;28mmax\u001b[39m(\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mlen\u001b[39m(iso_hiddens) \u001b[38;5;28;01mif\u001b[39;00m iso_hiddens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mlen\u001b[39m(can_hiddens) \u001b[38;5;28;01mif\u001b[39;00m can_hiddens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m     80\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mextract_representations_pairs.<locals>.run_encoder\u001b[39m\u001b[34m(txt)\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     70\u001b[39m inputs = tokenizer([txt], padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out.hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/ibm/MoLFormer-XL-both-10pct/7b12d946c181a37f6012b9dc3b002275de070314/modeling_molformer.py:682\u001b[39m, in \u001b[36mMolformerModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    678\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    680\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, inputs_embeds=inputs_embeds)\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    692\u001b[39m sequence_output = \u001b[38;5;28mself\u001b[39m.LayerNorm(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/ibm/MoLFormer-XL-both-10pct/7b12d946c181a37f6012b9dc3b002275de070314/modeling_molformer.py:426\u001b[39m, in \u001b[36mMolformerEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    418\u001b[39m     layer_outputs = torch.utils.checkpoint.checkpoint(\n\u001b[32m    419\u001b[39m         create_custom_forward(layer_module),\n\u001b[32m    420\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    423\u001b[39m         layer_head_mask,\n\u001b[32m    424\u001b[39m     )\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/ibm/MoLFormer-XL-both-10pct/7b12d946c181a37f6012b9dc3b002275de070314/modeling_molformer.py:361\u001b[39m, in \u001b[36mMolformerLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, head_mask, output_attentions)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    354\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    355\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    359\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    360\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    369\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/ibm/MoLFormer-XL-both-10pct/7b12d946c181a37f6012b9dc3b002275de070314/modeling_molformer.py:301\u001b[39m, in \u001b[36mMolformerAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, head_mask, output_attentions)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    294\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    295\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    299\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    300\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    309\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/ibm/MoLFormer-XL-both-10pct/7b12d946c181a37f6012b9dc3b002275de070314/modeling_molformer.py:200\u001b[39m, in \u001b[36mMolformerSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, head_mask, output_attentions)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    192\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    193\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    197\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    198\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    199\u001b[39m     query_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28mself\u001b[39m.query(hidden_states))\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     key_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    201\u001b[39m     value_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28mself\u001b[39m.value(hidden_states))\n\u001b[32m    203\u001b[39m     kv_seq_len = key_layer.shape[-\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm_olfaction/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "Input_types = {\n",
    "    'ibm/MoLFormer-XL-both-10pct':'smiles',\n",
    "    'seyonec/ChemBERTa-zinc-base-v1':'smiles',\n",
    "    'jonghyunlee/ChemBERT_ChEMBL_pretrained':'smiles',\n",
    "    \"HUBioDataLab/SELFormer\":'selfies'\n",
    "    }\n",
    "\n",
    "for model_name in ['ibm/MoLFormer-XL-both-10pct','seyonec/ChemBERTa-zinc-base-v1',\"jonghyunlee/ChemBERT_ChEMBL_pretrained\",\"HUBioDataLab/SELFormer\"]:\n",
    "    for ds in [\n",
    "        # 'sagar2023','keller2016','bierling2025',\n",
    "               'snitz2013']:\n",
    "    \n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True,use_safetensors=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        extract_representations_pairs(tokenizer, model,model_name,save_path='embeddings',ds=ds,input_type=Input_types[model_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "470b7f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/83 pairs...\n",
      "Saved cosine-similarity file â†’ ../datasets/snitz2013/snitz2013_data_MoLFormer-XL-both-10pct_cosine_from_embs_prefer_can.csv\n",
      "Processed 50/83 pairs...\n",
      "Saved cosine-similarity file â†’ ../datasets/snitz2013/snitz2013_data_ChemBERTa-zinc-base-v1_cosine_from_embs_prefer_can.csv\n",
      "Processed 50/83 pairs...\n",
      "Saved cosine-similarity file â†’ ../datasets/snitz2013/snitz2013_data_ChemBERT_ChEMBL_pretrained_cosine_from_embs_prefer_can.csv\n",
      "Processed 50/83 pairs...\n",
      "Saved cosine-similarity file â†’ ../datasets/snitz2013/snitz2013_data_SELFormer_cosine_from_embs_prefer_can.csv\n"
     ]
    }
   ],
   "source": [
    "for model_name in ['ibm/MoLFormer-XL-both-10pct','seyonec/ChemBERTa-zinc-base-v1',\"jonghyunlee/ChemBERT_ChEMBL_pretrained\",\"HUBioDataLab/SELFormer\"]:\n",
    "    for ds in [\n",
    "        # 'sagar2023','keller2016','bierling2025',\n",
    "               'snitz2013']:\n",
    "        out = build_cosine_similarity_from_embeddings(ds,model_name,\n",
    "            # embeddings_csv=f\"embeddings/{ds}_{model_name.split('/')[1]}_pair_embeddings.csv\",\n",
    "            layer=\"last\",\n",
    "            variant=\"prefer_can\",\n",
    "            out_csv=None,\n",
    "            overwrite_similarity=True,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0528b206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_olfaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
