{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T11:33:43.289449Z",
     "start_time": "2025-03-30T11:33:43.259703Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farzaneh/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer,AutoModelForMaskedLM, AutoModelForCausalLM,AutoModelForSeq2SeqLM,GraphormerForGraphClassification\n",
    "import pubchempy as pcp\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from rdkit import Chem\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../../../../T5 EVO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d78a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(seed=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430dacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_representations(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    model_name: str,   \n",
    "    ds: str,\n",
    "    input_type: str = \"smiles\",   # \"smiles\" or \"selfies\"\n",
    "    token: int = 0,               # which token to read (e.g., CLS/first)\n",
    "    save_path: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create embeddings from UNIQUE CIDs using either SMILES or SELFIES.\n",
    "    The output DF has:\n",
    "        ['cid', 'isomeric_text', 'canonical_text', 'input_type', 'model', 'layer', 'e0', ..., 'e{d-1}'].\n",
    "\n",
    "    Tokenization uses the chosen input_type; for each row it prefers the isomeric text if present,\n",
    "    otherwise falls back to the canonical text. Both text variants are kept as metadata columns.\n",
    "    \"\"\"\n",
    "    assert input_type.lower() in {\"smiles\", \"selfies\"}, \"input_type must be 'smiles' or 'selfies'\"\n",
    "    model.eval()\n",
    "\n",
    "    # ---- Load & pick columns ----\n",
    "    df = pd.read_csv(f\"datasets/{ds}/{ds}_data.csv\")\n",
    "    if \"cid\" not in df.columns:\n",
    "        raise ValueError(\"Dataset must contain a 'cid' column.\")\n",
    "\n",
    "    if input_type.lower() == \"smiles\":\n",
    "        iso_col = \"isomericsmiles\"\n",
    "        can_col = \"canonicalsmiles\"\n",
    "    else:  # selfies\n",
    "        iso_col = \"isomericselfies\"\n",
    "        can_col = \"canonicalselfies\"\n",
    "\n",
    "    if iso_col is None and can_col is None:\n",
    "        raise ValueError(f\"No {input_type} columns found.\")\n",
    "\n",
    "    use_cols = [\"cid\"]\n",
    "    if iso_col: use_cols.append(iso_col)\n",
    "    if can_col: use_cols.append(can_col)\n",
    "\n",
    "    work = df[use_cols].copy()\n",
    "    work = work.drop_duplicates(subset=[\"cid\"], keep=\"first\").sort_values(\"cid\").reset_index(drop=True)\n",
    "\n",
    "   \n",
    "    # ---- Device ----\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    rows = []\n",
    "    emb_dim = None\n",
    "\n",
    "    for cid, iso_txt, can_txt in zip(work[\"cid\"], work[iso_col], work[can_col]):\n",
    "        # --- normalize texts ---\n",
    "        iso_txt = iso_txt if isinstance(iso_txt, str) and iso_txt.strip() else \"\"\n",
    "        can_txt = can_txt if isinstance(can_txt, str) and can_txt.strip() else \"\"\n",
    "        if not iso_txt and not can_txt:\n",
    "            continue  # nothing to encode for this cid\n",
    "\n",
    "        # --- run model for isomeric (if present) ---\n",
    "        iso_hiddens = None\n",
    "        if iso_txt:\n",
    "            iso_inputs = tokenizer([iso_txt], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            iso_inputs = {k: v.to(device) for k, v in iso_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                iso_out = model(**iso_inputs, output_hidden_states=True)\n",
    "            iso_hiddens = iso_out.hidden_states  # tuple of [B,T,D] tensors\n",
    "\n",
    "        # --- run model for canonical (if present) ---\n",
    "        can_hiddens = None\n",
    "        if can_txt:\n",
    "            can_inputs = tokenizer([can_txt], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            can_inputs = {k: v.to(device) for k, v in can_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                can_out = model(**can_inputs, output_hidden_states=True)\n",
    "            can_hiddens = can_out.hidden_states\n",
    "\n",
    "        # number of layers to emit = max of the two (they should match)\n",
    "        n_layers = max(\n",
    "            len(iso_hiddens) if iso_hiddens is not None else 0,\n",
    "            len(can_hiddens) if can_hiddens is not None else 0,\n",
    "        )\n",
    "\n",
    "        for layer_idx in range(n_layers):\n",
    "            # get vectors (or None) for this layer\n",
    "           \n",
    "            iso_vec = iso_hiddens[layer_idx][0, token, :].detach().cpu().numpy()\n",
    "\n",
    "            \n",
    "            can_vec = can_hiddens[layer_idx][0, token, :].detach().cpu().numpy()\n",
    "\n",
    "            # set / check embedding dim\n",
    "            if emb_dim is None:\n",
    "                \n",
    "                emb_dim = iso_vec.shape[0]\n",
    "                emb_dim = can_vec.shape[0]\n",
    "            # sanity: if both exist, ensure same D\n",
    "            if (iso_vec is not None) and (can_vec is not None):\n",
    "                assert iso_vec.shape[0] == can_vec.shape[0], \"Iso/Can dims differ!\"\n",
    "\n",
    "            row = {\n",
    "                \"cid\": cid,\n",
    "                \"isomeric_text\": iso_txt,\n",
    "                \"canonical_text\": can_txt,\n",
    "                \"input_type\": input_type.lower(),   # \"smiles\" or \"selfies\"\n",
    "                \"model\": model_name,\n",
    "                \"layer\": layer_idx,\n",
    "            }\n",
    "\n",
    "            # add iso_* columns (fill with NaN if missing)\n",
    "            if emb_dim is None:\n",
    "                continue  # defensive; should not happen if any vec exists\n",
    "            for i in range(emb_dim):\n",
    "                row[f\"iso_e{i}\"] = float(iso_vec[i]) \n",
    "                row[f\"can_e{i}\"] = float(can_vec[i])\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "\n",
    "        out_df = pd.DataFrame(rows)\n",
    "\n",
    "        \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    out_df.to_csv(f\"{save_path}/{ds}_{model_name.split('/')[1]}_embeddings.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ab8fabf3601a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T07:32:48.485874Z",
     "start_time": "2025-03-31T07:32:48.439231Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_representations(\n",
    "    model,\n",
    "    model_name: str,   \n",
    "    ds: str,\n",
    "    input_type: str = \"smiles\",   # \"smiles\" or \"selfies\"\n",
    "    save_path: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create embeddings from UNIQUE CIDs using either SMILES or SELFIES.\n",
    "    The output DF has:\n",
    "        ['cid', 'isomeric_text', 'canonical_text', 'input_type', 'model', 'layer', 'e0', ..., 'e{d-1}'].\n",
    "\n",
    "    Tokenization uses the chosen input_type; for each row it prefers the isomeric text if present,\n",
    "    otherwise falls back to the canonical text. Both text variants are kept as metadata columns.\n",
    "    \"\"\"\n",
    "    assert input_type.lower() in {\"smiles\", \"selfies\"}, \"input_type must be 'smiles' or 'selfies'\"\n",
    "    model.eval()\n",
    "\n",
    "    # ---- Load & pick columns ----\n",
    "    df = pd.read_csv(f\"datasets/{ds}/{ds}_data.csv\")\n",
    "    if \"cid\" not in df.columns:\n",
    "        raise ValueError(\"Dataset must contain a 'cid' column.\")\n",
    "\n",
    "    if input_type.lower() == \"smiles\":\n",
    "        iso_col = \"isomericsmiles\"\n",
    "        can_col = \"canonicalsmiles\"\n",
    "    else:  # selfies\n",
    "        iso_col = \"isomericselfies\"\n",
    "        can_col = \"canonicalselfies\"\n",
    "\n",
    "    if iso_col is None and can_col is None:\n",
    "        raise ValueError(f\"No {input_type} columns found.\")\n",
    "\n",
    "    use_cols = [\"cid\"]\n",
    "    if iso_col: use_cols.append(iso_col)\n",
    "    if can_col: use_cols.append(can_col)\n",
    "\n",
    "    work = df[use_cols].copy()\n",
    "    work = work.drop_duplicates(subset=[\"cid\"], keep=\"first\").sort_values(\"cid\").reset_index(drop=True)\n",
    "\n",
    "   \n",
    "    # ---- Device ----\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    rows = []\n",
    "    emb_dim = None\n",
    "\n",
    "    for cid, iso_txt, can_txt in zip(work[\"cid\"], work[iso_col], work[can_col]):\n",
    "        # --- normalize texts ---\n",
    "        iso_txt = iso_txt if isinstance(iso_txt, str) and iso_txt.strip() else \"\"\n",
    "        can_txt = can_txt if isinstance(can_txt, str) and can_txt.strip() else \"\"\n",
    "        if not iso_txt and not can_txt:\n",
    "            continue  # nothing to encode for this cid\n",
    "\n",
    "        # --- run model for isomeric (if present) ---\n",
    "        iso_hiddens = None\n",
    "        if iso_txt:\n",
    "            \n",
    "            iso_out = model(iso_txt)\n",
    "            \n",
    "\n",
    "        # --- run model for canonical (if present) ---\n",
    "        \n",
    "        if can_txt:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                can_out = model(can_txt)\n",
    "            \n",
    "       \n",
    "       \n",
    "        if emb_dim is None:\n",
    "            \n",
    "            emb_dim = iso_out.shape[0]\n",
    "            emb_dim = can_out.shape[0]\n",
    "        # sanity: if both exist, ensure same D\n",
    "        if (iso_out is not None) and (can_out is not None):\n",
    "            assert iso_out.shape[0] == can_out.shape[0], \"Iso/Can dims differ!\"\n",
    "        row = {\n",
    "            \"cid\": cid,\n",
    "            \"isomeric_text\": iso_txt,\n",
    "            \"canonical_text\": can_txt,\n",
    "            \"input_type\": input_type.lower(),   # \"smiles\" or \"selfies\"\n",
    "            \"model\": model_name,\n",
    "            \n",
    "        }\n",
    "        # add iso_* columns (fill with NaN if missing)\n",
    "        if emb_dim is None:\n",
    "            continue  # defensive; should not happen if any vec exists\n",
    "        for i in range(emb_dim):\n",
    "            row[f\"iso_e{i}\"] = float(iso_out[i]) \n",
    "            row[f\"can_e{i}\"] = float(can_out[i])\n",
    "        rows.append(row)\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "\n",
    "        \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    out_df.to_csv(f\"{save_path}/{ds}_{model_name.split('/')[1]}_embeddings.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca77193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_representations_by_molfeat(model_name,transformer,input_type='smiles',token=0):\n",
    "    \n",
    "#     for subject_id in range(s_start,s_end+1):\n",
    "#         input_molecules = pd.read_csv(f'{base_dir}/fmri/embeddings{ds}/CIDs_smiles_selfies_{subject_id}{ds}.csv')[input_type].values.tolist()\n",
    "        \n",
    "#         outputs = transformer(input_molecules)\n",
    "#         np.save(f'{base_dir}/fmri/embeddings{ds}/embeddings_{model_name}_{subject_id}_{-1}{ds}.npy',outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e1f67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e49e21f0d63ede",
   "metadata": {},
   "source": [
    "# Encoder-Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c357405615704",
   "metadata": {},
   "source": [
    "## MoLFormer-XL-both-10pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9682defb4c9030c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T21:03:57.047530Z",
     "start_time": "2025-01-06T21:00:03.803379Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/var/folders/s8/bznqh0v13ddg31lz919tkrmc0000gn/T/ipykernel_37065/1033378615.py:22: DtypeWarning: Columns (166) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"datasets/{ds}/{ds}_data.csv\")\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/var/folders/s8/bznqh0v13ddg31lz919tkrmc0000gn/T/ipykernel_37065/1033378615.py:22: DtypeWarning: Columns (166) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"datasets/{ds}/{ds}_data.csv\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jonghyunlee/ChemBERT_ChEMBL_pretrained and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jonghyunlee/ChemBERT_ChEMBL_pretrained and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jonghyunlee/ChemBERT_ChEMBL_pretrained and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/s8/bznqh0v13ddg31lz919tkrmc0000gn/T/ipykernel_37065/1033378615.py:22: DtypeWarning: Columns (166) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"datasets/{ds}/{ds}_data.csv\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at HUBioDataLab/SELFormer and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at HUBioDataLab/SELFormer and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at HUBioDataLab/SELFormer and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/s8/bznqh0v13ddg31lz919tkrmc0000gn/T/ipykernel_37065/1033378615.py:22: DtypeWarning: Columns (166) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"datasets/{ds}/{ds}_data.csv\")\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "Input_types = {\n",
    "    'ibm/MoLFormer-XL-both-10pct':'smiles',\n",
    "    'seyonec/ChemBERTa-zinc-base-v1':'smiles',\n",
    "    'jonghyunlee/ChemBERT_ChEMBL_pretrained':'smiles',\n",
    "    \"HUBioDataLab/SELFormer\":'selfies'\n",
    "    }\n",
    "\n",
    "for model_name in ['ibm/MoLFormer-XL-both-10pct','seyonec/ChemBERTa-zinc-base-v1',\"jonghyunlee/ChemBERT_ChEMBL_pretrained\",\"HUBioDataLab/SELFormer\"]:\n",
    "    for ds in ['sagar2023','keller2016','bierling2025']:\n",
    "    \n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        extract_representations(tokenizer, model,model_name,save_path='embeddings',ds=ds,input_type=Input_types[model_name])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoLFormer_fMRI",
   "language": "python",
   "name": "molformer_fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
